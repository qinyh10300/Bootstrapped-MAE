diff --git a/.gitignore b/.gitignore
deleted file mode 100644
index a46ad83..0000000
--- a/.gitignore
+++ /dev/null
@@ -1,6 +0,0 @@
-dataset/cifar10_dataset/
-ckpts/
-logs/
-__pycache__/
-util/__pycache__/
-experiments/hyperparam_results/
\ No newline at end of file
diff --git a/README_BMAE.md b/README_BMAE.md
deleted file mode 100644
index dd08cc7..0000000
--- a/README_BMAE.md
+++ /dev/null
@@ -1,17 +0,0 @@
-
-
-在 `bash` 中加载 YAML 文件并解析其中的参数并不是一件直接的事，因为 `bash` 本身并不支持直接解析 YAML 格式。
-
-`yq` 是一个非常流行的命令行工具，可以用来处理 YAML 文件，详细内容可以参考[这个仓库](https://github.com/mikefarah/yq)。它类似于 `jq`（用于 JSON），但专门用于 YAML 格式。
-
-```
-sudo snap install yq
-```
-
-`yq` 在处理 YAML 文件时，实际上是依赖于 `jq` 来进行 JSON 处理的。
-
-如果你没有安装`jq`，可以运行以下命令：
-
-```
-sudo apt-get install jq
-```
\ No newline at end of file
diff --git a/__pycache__/engine_pretrain.cpython-39.pyc b/__pycache__/engine_pretrain.cpython-39.pyc
deleted file mode 100644
index 6b1faba..0000000
Binary files a/__pycache__/engine_pretrain.cpython-39.pyc and /dev/null differ
diff --git a/__pycache__/models_mae.cpython-39.pyc b/__pycache__/models_mae.cpython-39.pyc
deleted file mode 100644
index 1b0b92a..0000000
Binary files a/__pycache__/models_mae.cpython-39.pyc and /dev/null differ
diff --git a/bash_scripts/Bmae_eval_finetune.sh b/bash_scripts/Bmae_eval_finetune.sh
deleted file mode 100644
index 4aba8bd..0000000
--- a/bash_scripts/Bmae_eval_finetune.sh
+++ /dev/null
@@ -1,132 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-# 默认值设置
-NAME="Bmae_deit_finetune"
-MODEL="deit_tiny_patch4"
-DATA_PATH="./dataset/cifar10_dataset"
-BATCH_SIZE=256
-EPOCHS=100
-WARMUP_EPOCHS=10
-BASE_LR=1e-3
-INPUT_SIZE=32
-WEIGHT_DECAY=0
-DROP_PATH=0.05
-CKPT="ckpts/Bmae_train_deit/pretrained/Bmae-5_EMA-39.pth"
-DEVICE="cuda:0"
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-SAVE_FREQUENCY=20
-
-# 解析命令行参数
-while [[ $# -gt 0 ]]; do
-  case $1 in
-    --name)
-      NAME="$2"
-      shift 2
-      ;;
-    --model)
-      MODEL="$2"
-      shift 2
-      ;;
-    --data_path)
-      DATA_PATH="$2"
-      shift 2
-      ;;
-    --batch_size)
-      BATCH_SIZE="$2"
-      shift 2
-      ;;
-    --epochs)
-      EPOCHS="$2"
-      shift 2
-      ;;
-    --warmup_epochs)
-      WARMUP_EPOCHS="$2"
-      shift 2
-      ;;
-    --blr)
-      BASE_LR="$2"
-      shift 2
-      ;;
-    --input_size)
-      INPUT_SIZE="$2"
-      shift 2
-      ;;
-    --weight_decay)
-      WEIGHT_DECAY="$2"
-      shift 2
-      ;;
-    --drop_path)
-      DROP_PATH="$2"
-      shift 2
-      ;;
-    --finetune)
-      CKPT="$2"
-      shift 2
-      ;;
-    --device)
-      DEVICE="$2"
-      shift 2
-      ;;
-    --current_datetime)
-      CURRENT_DATETIME="$2"
-      shift 2
-      ;;
-    --save_frequency)
-      SAVE_FREQUENCY="$2"
-      shift 2
-      ;;
-    *)
-      echo "Unknown argument: $1"
-      exit 1
-      ;;
-  esac
-done
-
-# 动态生成日志和输出目录
-LOG_DIR="./logs/${NAME}/tb_${CURRENT_DATETIME}"
-OUTPUT_DIR="./ckpts/${NAME}/${CURRENT_DATETIME}"
-
-# 创建日志目录
-mkdir -p ${LOG_DIR}
-
-# 将参数写入 YAML 格式的参数表文件
-PARAMS_FILE="${LOG_DIR}/params.yaml"
-echo "name: ${NAME}" > ${PARAMS_FILE}
-echo "model: ${MODEL}" >> ${PARAMS_FILE}
-echo "data_path: ${DATA_PATH}" >> ${PARAMS_FILE}
-echo "batch_size: ${BATCH_SIZE}" >> ${PARAMS_FILE}
-echo "epochs: ${EPOCHS}" >> ${PARAMS_FILE}
-echo "warmup_epochs: ${WARMUP_EPOCHS}" >> ${PARAMS_FILE}
-echo "base_lr: ${BASE_LR}" >> ${PARAMS_FILE}
-echo "input_size: ${INPUT_SIZE}" >> ${PARAMS_FILE}
-echo "weight_decay: ${WEIGHT_DECAY}" >> ${PARAMS_FILE}
-echo "drop_path: ${DROP_PATH}" >> ${PARAMS_FILE}
-echo "finetune: ${CKPT}" >> ${PARAMS_FILE}
-echo "device: ${DEVICE}" >> ${PARAMS_FILE}
-echo "log_dir: ${LOG_DIR}" >> ${PARAMS_FILE}
-echo "output_dir: ${OUTPUT_DIR}" >> ${PARAMS_FILE}
-echo "current_datetime: ${CURRENT_DATETIME}" >> ${PARAMS_FILE}
-echo "save_frequency: ${SAVE_FREQUENCY}" >> ${PARAMS_FILE}
-
-# 执行 Python 脚本
-python main_finetune.py \
-    --name ${NAME} \
-    --model ${MODEL} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --input_size ${INPUT_SIZE} \
-    --weight_decay ${WEIGHT_DECAY} \
-    --drop_path ${DROP_PATH} \
-    --log_dir ${LOG_DIR} \
-    --finetune ${CKPT} \
-    --nb_classes 10 \
-    --mixup 0.8 \
-    --cutmix 1.0 \
-    --current_datetime ${CURRENT_DATETIME} \
-    --device ${DEVICE} \
-    --save_frequency ${SAVE_FREQUENCY}
\ No newline at end of file
diff --git a/bash_scripts/Bmae_eval_linear.sh b/bash_scripts/Bmae_eval_linear.sh
deleted file mode 100644
index 83dd683..0000000
--- a/bash_scripts/Bmae_eval_linear.sh
+++ /dev/null
@@ -1,123 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-# 默认值设置
-NAME="Bmae_deit_linear"
-MODEL="deit_tiny_patch4"
-DATA_PATH="./dataset/cifar10_dataset"
-BATCH_SIZE=256  # 修改以适配 GPU 内存
-EPOCHS=100
-WARMUP_EPOCHS=10
-BASE_LR=0.01
-WEIGHT_DECAY=0
-INPUT_SIZE=32
-CKPT="ckpts/Bmae_train_deit/pretrained/Bmae-5_EMA-39.pth"
-DEVICE="cuda:1"
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-SAVE_FREQUENCY=20
-
-# 解析命令行参数
-while [[ $# -gt 0 ]]; do
-  case $1 in
-    --name)
-      NAME="$2"
-      shift 2
-      ;;
-    --model)
-      MODEL="$2"
-      shift 2
-      ;;
-    --data_path)
-      DATA_PATH="$2"
-      shift 2
-      ;;
-    --batch_size)
-      BATCH_SIZE="$2"
-      shift 2
-      ;;
-    --epochs)
-      EPOCHS="$2"
-      shift 2
-      ;;
-    --warmup_epochs)
-      WARMUP_EPOCHS="$2"
-      shift 2
-      ;;
-    --blr)
-      BASE_LR="$2"
-      shift 2
-      ;;
-    --weight_decay)
-      WEIGHT_DECAY="$2"
-      shift 2
-      ;;
-    --input_size)
-      INPUT_SIZE="$2"
-      shift 2
-      ;;
-    --finetune)
-      CKPT="$2"
-      shift 2
-      ;;
-    --device)
-      DEVICE="$2"
-      shift 2
-      ;;
-    --current_datetime)
-      CURRENT_DATETIME="$2"
-      shift 2
-      ;;
-    --save_frequency)
-      SAVE_FREQUENCY="$2"
-      shift 2
-      ;;
-    *)
-      echo "Unknown argument: $1"
-      exit 1
-      ;;
-  esac
-done
-
-# 动态生成日志和输出目录
-LOG_DIR="./logs/${NAME}/tb_${CURRENT_DATETIME}"
-OUTPUT_DIR="./ckpts/${NAME}/${CURRENT_DATETIME}"
-
-# 创建日志目录
-mkdir -p ${LOG_DIR}
-
-# 将参数写入 YAML 格式的参数表文件
-PARAMS_FILE="${LOG_DIR}/params.yaml"
-echo "name: ${NAME}" > ${PARAMS_FILE}
-echo "model: ${MODEL}" >> ${PARAMS_FILE}
-echo "data_path: ${DATA_PATH}" >> ${PARAMS_FILE}
-echo "batch_size: ${BATCH_SIZE}" >> ${PARAMS_FILE}
-echo "epochs: ${EPOCHS}" >> ${PARAMS_FILE}
-echo "warmup_epochs: ${WARMUP_EPOCHS}" >> ${PARAMS_FILE}
-echo "base_lr: ${BASE_LR}" >> ${PARAMS_FILE}
-echo "weight_decay: ${WEIGHT_DECAY}" >> ${PARAMS_FILE}
-echo "input_size: ${INPUT_SIZE}" >> ${PARAMS_FILE}
-echo "finetune: ${CKPT}" >> ${PARAMS_FILE}
-echo "device: ${DEVICE}" >> ${PARAMS_FILE}
-echo "log_dir: ${LOG_DIR}" >> ${PARAMS_FILE}
-echo "output_dir: ${OUTPUT_DIR}" >> ${PARAMS_FILE}
-echo "current_datetime: ${CURRENT_DATETIME}" >> ${PARAMS_FILE}
-echo "save_frequency: ${SAVE_FREQUENCY}" >> ${PARAMS_FILE}
-
-# 执行 Python 脚本
-python main_linprobe.py \
-    --name ${NAME} \
-    --model ${MODEL} \
-    --input_size ${INPUT_SIZE} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --weight_decay ${WEIGHT_DECAY} \
-    --log_dir ${LOG_DIR} \
-    --finetune ${CKPT} \
-    --nb_classes 10 \
-    --device ${DEVICE} \
-    --current_datetime ${CURRENT_DATETIME} \
-    --save_frequency ${SAVE_FREQUENCY}
\ No newline at end of file
diff --git a/bash_scripts/Bmae_train.sh b/bash_scripts/Bmae_train.sh
deleted file mode 100644
index 7d84c58..0000000
--- a/bash_scripts/Bmae_train.sh
+++ /dev/null
@@ -1,165 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-# 默认值设置
-NAME="Bmae_deit_pretrain"
-MODEL="mae_deit_tiny_patch4"
-DATA_PATH="./dataset/cifar10_dataset"
-BATCH_SIZE=256
-ACCUM=2
-EPOCHS=200
-WARMUP_EPOCHS=10
-BASE_LR=1.5e-4
-INPUT_SIZE=32
-MASK_RATIO=0.75
-BOOTSTRAP_STEPS=5
-BOOTSTRAP_METHOD='Last_layer'
-EMA_DECAY=0.99
-DEVICE="cuda:0"
-USE_EMA=false  # 默认不使用 EMA
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-SAVE_FREQUENCY=20
-NORM_PIX_LOSS=true  # 默认不使用 norm_pix_loss
-OPTIM='AdamW'
-
-# 解析命令行参数
-while [[ $# -gt 0 ]]; do
-  case $1 in
-    --name)
-      NAME="$2"
-      shift 2
-      ;;
-    --model)
-      MODEL="$2"
-      shift 2
-      ;;
-    --data_path)
-      DATA_PATH="$2"
-      shift 2
-      ;;
-    --batch_size)
-      BATCH_SIZE="$2"
-      shift 2
-      ;;
-    --accum_iter)
-      ACCUM="$2"
-      shift 2
-      ;;
-    --epochs)
-      EPOCHS="$2"
-      shift 2
-      ;;
-    --warmup_epochs)
-      WARMUP_EPOCHS="$2"
-      shift 2
-      ;;
-    --blr)
-      BASE_LR="$2"
-      shift 2
-      ;;
-    --input_size)
-      INPUT_SIZE="$2"
-      shift 2
-      ;;
-    --mask_ratio)
-      MASK_RATIO="$2"
-      shift 2
-      ;;
-    --bootstrap_steps)
-      BOOTSTRAP_STEPS="$2"
-      shift 2
-      ;;
-    --bootstrap_method)
-      BOOTSTRAP_METHOD="$2"
-      shift 2
-      ;;
-    --ema_decay)
-      EMA_DECAY="$2"
-      shift 2
-      ;;
-    --device)
-      DEVICE="$2"
-      shift 2
-      ;;
-    --use_ema)
-      USE_EMA=true  # 如果传入 --use_ema，则启用 EMA
-      shift 1
-      ;;
-    --current_datetime)
-      CURRENT_DATETIME="$2"
-      shift 2
-      ;;
-    --save_frequency)
-      SAVE_FREQUENCY="$2"
-      shift 2
-      ;;
-    --norm_pix_loss)
-      NORM_PIX_LOSS=true  # 如果传入 --use_ema，则启用 EMA
-      shift 1
-      ;;
-    --optim)
-      OPTIM="$2"
-      shift 2
-      ;;
-    *)
-      echo "Unknown argument: $1"
-      exit 1
-      ;;
-  esac
-done
-
-# 动态生成日志和输出目录
-LOG_DIR="./logs/${NAME}/tb_${CURRENT_DATETIME}"
-OUTPUT_DIR="./ckpts/${NAME}/${CURRENT_DATETIME}"
-
-# 创建日志目录
-mkdir -p ${LOG_DIR}
-
-# 将参数写入 YAML 格式的参数表文件
-PARAMS_FILE="${LOG_DIR}/params.yaml"
-echo "name: ${NAME}" > ${PARAMS_FILE}
-echo "model: ${MODEL}" >> ${PARAMS_FILE}
-echo "data_path: ${DATA_PATH}" >> ${PARAMS_FILE}
-echo "batch_size: ${BATCH_SIZE}" >> ${PARAMS_FILE}
-echo "accum_iter: ${ACCUM}" >> ${PARAMS_FILE}
-echo "epochs: ${EPOCHS}" >> ${PARAMS_FILE}
-echo "warmup_epochs: ${WARMUP_EPOCHS}" >> ${PARAMS_FILE}
-echo "base_lr: ${BASE_LR}" >> ${PARAMS_FILE}
-echo "input_size: ${INPUT_SIZE}" >> ${PARAMS_FILE}
-echo "mask_ratio: ${MASK_RATIO}" >> ${PARAMS_FILE}
-echo "bootstrap_steps: ${BOOTSTRAP_STEPS}" >> ${PARAMS_FILE}
-echo "bootstrap_method: ${BOOTSTRAP_METHOD}" >> ${PARAMS_FILE}
-echo "ema_decay: ${EMA_DECAY}" >> ${PARAMS_FILE}
-echo "device: ${DEVICE}" >> ${PARAMS_FILE}
-echo "use_ema: ${USE_EMA}" >> ${PARAMS_FILE}
-echo "log_dir: ${LOG_DIR}" >> ${PARAMS_FILE}
-echo "output_dir: ${OUTPUT_DIR}" >> ${PARAMS_FILE}
-echo "current_datetime: ${CURRENT_DATETIME}" >> ${PARAMS_FILE}
-echo "save_frequency: ${SAVE_FREQUENCY}" >> ${PARAMS_FILE}
-echo "norm_pix_loss: ${NORM_PIX_LOSS}" >> ${PARAMS_FILE}
-echo "optim: ${OPTIM}" >> ${PARAMS_FILE}
-
-# 执行 Python 脚本
-python main_pretrain.py \
-    --name ${NAME} \
-    --model ${MODEL} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --accum_iter ${ACCUM} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --input_size ${INPUT_SIZE} \
-    --mask_ratio ${MASK_RATIO} \
-    $( [ "${NORM_PIX_LOSS}" = true ] && echo "--norm_pix_loss" ) \
-    --log_dir ${LOG_DIR} \
-    --is_bootstrapping \
-    --bootstrap_steps ${BOOTSTRAP_STEPS} \
-    --bootstrap_method ${BOOTSTRAP_METHOD} \
-    $( [ "${USE_EMA}" = true ] && echo "--use_ema" ) \
-    --ema_decay ${EMA_DECAY} \
-    --device ${DEVICE} \
-    --current_datetime ${CURRENT_DATETIME} \
-    --save_frequency ${SAVE_FREQUENCY} \
-    --optim ${OPTIM} \
\ No newline at end of file
diff --git a/bash_scripts/mae_eval_finetune.sh b/bash_scripts/mae_eval_finetune.sh
deleted file mode 100644
index 6694ea3..0000000
--- a/bash_scripts/mae_eval_finetune.sh
+++ /dev/null
@@ -1,125 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-# 默认值设置
-NAME="mae_deit_finetune"
-MODEL="deit_tiny_patch4"
-DATA_PATH="./dataset/cifar10_dataset"
-BATCH_SIZE=256
-EPOCHS=100
-WARMUP_EPOCHS=10
-BASE_LR=1e-3
-INPUT_SIZE=32
-WEIGHT_DECAY=0
-DROP_PATH=0.05
-CKPT="ckpts/mae_2025-04-26_16-56-35-199.pth"
-DEVICE="cuda:0"
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-
-# 解析命令行参数
-while [[ $# -gt 0 ]]; do
-  case $1 in
-    --name)
-      NAME="$2"
-      shift 2
-      ;;
-    --model)
-      MODEL="$2"
-      shift 2
-      ;;
-    --data_path)
-      DATA_PATH="$2"
-      shift 2
-      ;;
-    --batch_size)
-      BATCH_SIZE="$2"
-      shift 2
-      ;;
-    --epochs)
-      EPOCHS="$2"
-      shift 2
-      ;;
-    --warmup_epochs)
-      WARMUP_EPOCHS="$2"
-      shift 2
-      ;;
-    --blr)
-      BASE_LR="$2"
-      shift 2
-      ;;
-    --input_size)
-      INPUT_SIZE="$2"
-      shift 2
-      ;;
-    --weight_decay)
-      WEIGHT_DECAY="$2"
-      shift 2
-      ;;
-    --drop_path)
-      DROP_PATH="$2"
-      shift 2
-      ;;
-    --finetune)
-      CKPT="$2"
-      shift 2
-      ;;
-    --device)
-      DEVICE="$2"
-      shift 2
-      ;;
-    --current_datetime)
-      CURRENT_DATETIME="$2"
-      shift 2
-      ;;
-    *)
-      echo "Unknown argument: $1"
-      exit 1
-      ;;
-  esac
-done
-
-# 动态生成日志和输出目录
-LOG_DIR="./logs/${NAME}/tb_${CURRENT_DATETIME}"
-OUTPUT_DIR="./ckpts/${NAME}/${CURRENT_DATETIME}"
-
-# 创建日志目录
-mkdir -p ${LOG_DIR}
-
-# 将参数写入 YAML 格式的参数表文件
-PARAMS_FILE="${LOG_DIR}/params.yaml"
-echo "name: ${NAME}" > ${PARAMS_FILE}
-echo "model: ${MODEL}" >> ${PARAMS_FILE}
-echo "data_path: ${DATA_PATH}" >> ${PARAMS_FILE}
-echo "batch_size: ${BATCH_SIZE}" >> ${PARAMS_FILE}
-echo "epochs: ${EPOCHS}" >> ${PARAMS_FILE}
-echo "warmup_epochs: ${WARMUP_EPOCHS}" >> ${PARAMS_FILE}
-echo "base_lr: ${BASE_LR}" >> ${PARAMS_FILE}
-echo "input_size: ${INPUT_SIZE}" >> ${PARAMS_FILE}
-echo "weight_decay: ${WEIGHT_DECAY}" >> ${PARAMS_FILE}
-echo "drop_path: ${DROP_PATH}" >> ${PARAMS_FILE}
-echo "finetune: ${CKPT}" >> ${PARAMS_FILE}
-echo "device: ${DEVICE}" >> ${PARAMS_FILE}
-echo "log_dir: ${LOG_DIR}" >> ${PARAMS_FILE}
-echo "output_dir: ${OUTPUT_DIR}" >> ${PARAMS_FILE}
-echo "current_datetime: ${CURRENT_DATETIME}" >> ${PARAMS_FILE}
-
-# 执行 Python 脚本
-python main_finetune.py \
-    --name ${NAME} \
-    --model ${MODEL} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --input_size ${INPUT_SIZE} \
-    --weight_decay ${WEIGHT_DECAY} \
-    --drop_path ${DROP_PATH} \
-    --log_dir ${LOG_DIR} \
-    --finetune ${CKPT} \
-    --nb_classes 10 \
-    --mixup 0.8 \
-    --cutmix 1.0 \
-    --current_datetime ${CURRENT_DATETIME} \
-    --device ${DEVICE}
\ No newline at end of file
diff --git a/bash_scripts/mae_eval_linear.sh b/bash_scripts/mae_eval_linear.sh
deleted file mode 100644
index 8fa41c3..0000000
--- a/bash_scripts/mae_eval_linear.sh
+++ /dev/null
@@ -1,116 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-# 默认值设置
-NAME="mae_deit_linear"
-MODEL="deit_tiny_patch4"
-DATA_PATH="./dataset/cifar10_dataset"
-BATCH_SIZE=256  # 修改以适配 GPU 内存
-EPOCHS=100
-WARMUP_EPOCHS=10
-BASE_LR=0.01
-WEIGHT_DECAY=0
-INPUT_SIZE=32
-CKPT="ckpts/mae_2025-04-26_16-56-35-199.pth"
-DEVICE="cuda:0"
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-
-# 解析命令行参数
-while [[ $# -gt 0 ]]; do
-  case $1 in
-    --name)
-      NAME="$2"
-      shift 2
-      ;;
-    --model)
-      MODEL="$2"
-      shift 2
-      ;;
-    --data_path)
-      DATA_PATH="$2"
-      shift 2
-      ;;
-    --batch_size)
-      BATCH_SIZE="$2"
-      shift 2
-      ;;
-    --epochs)
-      EPOCHS="$2"
-      shift 2
-      ;;
-    --warmup_epochs)
-      WARMUP_EPOCHS="$2"
-      shift 2
-      ;;
-    --blr)
-      BASE_LR="$2"
-      shift 2
-      ;;
-    --weight_decay)
-      WEIGHT_DECAY="$2"
-      shift 2
-      ;;
-    --input_size)
-      INPUT_SIZE="$2"
-      shift 2
-      ;;
-    --finetune)
-      CKPT="$2"
-      shift 2
-      ;;
-    --device)
-      DEVICE="$2"
-      shift 2
-      ;;
-    --current_datetime)
-      CURRENT_DATETIME="$2"
-      shift 2
-      ;;
-    *)
-      echo "Unknown argument: $1"
-      exit 1
-      ;;
-  esac
-done
-
-# 动态生成日志和输出目录
-LOG_DIR="./logs/${NAME}/tb_${CURRENT_DATETIME}"
-OUTPUT_DIR="./ckpts/${NAME}/${CURRENT_DATETIME}"
-
-# 创建日志目录
-mkdir -p ${LOG_DIR}
-
-# 将参数写入 YAML 格式的参数表文件
-PARAMS_FILE="${LOG_DIR}/params.yaml"
-echo "name: ${NAME}" > ${PARAMS_FILE}
-echo "model: ${MODEL}" >> ${PARAMS_FILE}
-echo "data_path: ${DATA_PATH}" >> ${PARAMS_FILE}
-echo "batch_size: ${BATCH_SIZE}" >> ${PARAMS_FILE}
-echo "epochs: ${EPOCHS}" >> ${PARAMS_FILE}
-echo "warmup_epochs: ${WARMUP_EPOCHS}" >> ${PARAMS_FILE}
-echo "base_lr: ${BASE_LR}" >> ${PARAMS_FILE}
-echo "weight_decay: ${WEIGHT_DECAY}" >> ${PARAMS_FILE}
-echo "input_size: ${INPUT_SIZE}" >> ${PARAMS_FILE}
-echo "finetune: ${CKPT}" >> ${PARAMS_FILE}
-echo "device: ${DEVICE}" >> ${PARAMS_FILE}
-echo "log_dir: ${LOG_DIR}" >> ${PARAMS_FILE}
-echo "output_dir: ${OUTPUT_DIR}" >> ${PARAMS_FILE}
-echo "current_datetime: ${CURRENT_DATETIME}" >> ${PARAMS_FILE}
-
-# 执行 Python 脚本
-python main_linprobe.py \
-    --name ${NAME} \
-    --model ${MODEL} \
-    --input_size ${INPUT_SIZE} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --weight_decay ${WEIGHT_DECAY} \
-    --log_dir ${LOG_DIR} \
-    --finetune ${CKPT} \
-    --nb_classes 10 \
-    --device ${DEVICE} \
-    --current_datetime ${CURRENT_DATETIME}
\ No newline at end of file
diff --git a/bash_scripts/mae_train.sh b/bash_scripts/mae_train.sh
deleted file mode 100644
index a869d82..0000000
--- a/bash_scripts/mae_train.sh
+++ /dev/null
@@ -1,116 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-# 默认值设置
-NAME="mae_deit_pretrain"
-MODEL="mae_deit_tiny_patch4"
-DATA_PATH="./dataset/cifar10_dataset"
-BATCH_SIZE=256
-ACCUM=2
-EPOCHS=200
-WARMUP_EPOCHS=10
-BASE_LR=1.5e-4
-INPUT_SIZE=32
-MASK_RATIO=0.75
-DEVICE="cuda:0"
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-
-# 解析命令行参数
-while [[ $# -gt 0 ]]; do
-  case $1 in
-    --name)
-      NAME="$2"
-      shift 2
-      ;;
-    --model)
-      MODEL="$2"
-      shift 2
-      ;;
-    --data_path)
-      DATA_PATH="$2"
-      shift 2
-      ;;
-    --batch_size)
-      BATCH_SIZE="$2"
-      shift 2
-      ;;
-    --accum_iter)
-      ACCUM="$2"
-      shift 2
-      ;;
-    --epochs)
-      EPOCHS="$2"
-      shift 2
-      ;;
-    --warmup_epochs)
-      WARMUP_EPOCHS="$2"
-      shift 2
-      ;;
-    --blr)
-      BASE_LR="$2"
-      shift 2
-      ;;
-    --input_size)
-      INPUT_SIZE="$2"
-      shift 2
-      ;;
-    --mask_ratio)
-      MASK_RATIO="$2"
-      shift 2
-      ;;
-    --device)
-      DEVICE="$2"
-      shift 2
-      ;;
-    --current_datetime)
-      CURRENT_DATETIME="$2"
-      shift 2
-      ;;
-    *)
-      echo "Unknown argument: $1"
-      exit 1
-      ;;
-  esac
-done
-
-# 动态生成日志和输出目录
-LOG_DIR="./logs/${NAME}/tb_${CURRENT_DATETIME}"
-OUTPUT_DIR="./ckpts/${NAME}/${CURRENT_DATETIME}"
-
-# 创建日志目录
-mkdir -p ${LOG_DIR}
-
-# 将参数写入 YAML 格式的参数表文件
-PARAMS_FILE="${LOG_DIR}/params.yaml"
-echo "name: ${NAME}" > ${PARAMS_FILE}
-echo "model: ${MODEL}" >> ${PARAMS_FILE}
-echo "data_path: ${DATA_PATH}" >> ${PARAMS_FILE}
-echo "batch_size: ${BATCH_SIZE}" >> ${PARAMS_FILE}
-echo "accum_iter: ${ACCUM}" >> ${PARAMS_FILE}
-echo "epochs: ${EPOCHS}" >> ${PARAMS_FILE}
-echo "warmup_epochs: ${WARMUP_EPOCHS}" >> ${PARAMS_FILE}
-echo "base_lr: ${BASE_LR}" >> ${PARAMS_FILE}
-echo "input_size: ${INPUT_SIZE}" >> ${PARAMS_FILE}
-echo "mask_ratio: ${MASK_RATIO}" >> ${PARAMS_FILE}
-echo "device: ${DEVICE}" >> ${PARAMS_FILE}
-echo "log_dir: ${LOG_DIR}" >> ${PARAMS_FILE}
-echo "output_dir: ${OUTPUT_DIR}" >> ${PARAMS_FILE}
-echo "current_datetime: ${CURRENT_DATETIME}" >> ${PARAMS_FILE}
-
-# 执行 Python 脚本
-python main_pretrain.py \
-    --name ${NAME} \
-    --model ${MODEL} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --accum_iter ${ACCUM} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --input_size ${INPUT_SIZE} \
-    --mask_ratio ${MASK_RATIO} \
-    --norm_pix_loss \
-    --log_dir ${LOG_DIR} \
-    --device ${DEVICE} \
-    --current_datetime ${CURRENT_DATETIME}
\ No newline at end of file
diff --git a/bash_scripts_discard/configs/default_mae_deit_tiny.yaml b/bash_scripts_discard/configs/default_mae_deit_tiny.yaml
deleted file mode 100644
index fe6b291..0000000
--- a/bash_scripts_discard/configs/default_mae_deit_tiny.yaml
+++ /dev/null
@@ -1,13 +0,0 @@
-NUM_GPUS: 2
-NAME: mae_train_deit
-MODEL: mae_deit_tiny_patch4
-DATA_PATH: ./dataset/cifar10_dataset
-OUTPUT_DIR: ./ckpts/original_mae/pretrained
-BATCH_SIZE: 64
-ACCUM: 2
-EPOCHS: 200
-WARMUP_EPOCHS: 10
-BASE_LR: 1.5e-4
-INPUT_SIZE: 32
-MASK_RATIO: 0.75
-RESUME: ./ckpts/original_mae/pretrained/checkpoint-20.pth
\ No newline at end of file
diff --git a/bash_scripts_discard/configs/mae_vit_tiny.yaml b/bash_scripts_discard/configs/mae_vit_tiny.yaml
deleted file mode 100644
index 87ada94..0000000
--- a/bash_scripts_discard/configs/mae_vit_tiny.yaml
+++ /dev/null
@@ -1,13 +0,0 @@
-NUM_GPUS: 2
-NAME: mae_train_vit
-MODEL: mae_vit_tiny_patch4
-DATA_PATH: ./dataset/cifar10_dataset
-OUTPUT_DIR: ./ckpts/original_mae/pretrained
-BATCH_SIZE: 64
-ACCUM: 2
-EPOCHS: 200
-WARMUP_EPOCHS: 10
-BASE_LR: 1.5e-4
-INPUT_SIZE: 32
-MASK_RATIO: 0.75
-RESUME: ./ckpts/original_mae/pretrained/checkpoint-20.pth
\ No newline at end of file
diff --git a/bash_scripts_discard/mae_train (Discard).sh b/bash_scripts_discard/mae_train (Discard).sh
deleted file mode 100644
index 7952cbd..0000000
--- a/bash_scripts_discard/mae_train (Discard).sh	
+++ /dev/null
@@ -1,76 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-# 默认 YAML 配置文件路径
-DEFAULT_CONFIG_FILE="./experiments/default_mae_deit_tiny.yaml"
-
-# 解析命令行参数
-TEMP=$(getopt -o "c:" --long cfg: -n 'mae_train.sh' -- "$@")
-if [ $? != 0 ]; then
-    echo "Error parsing options."
-    exit 1
-fi
-
-# 设置解析后的参数
-eval set -- "$TEMP"
-
-# 初始化变量
-CONFIG_FILE=$DEFAULT_CONFIG_FILE
-
-# 处理命令行参数
-while true; do
-    case "$1" in
-        -c|--cfg)
-            CONFIG_FILE="$2"
-            shift 2
-            ;;
-        --)
-            shift
-            break
-            ;;
-        *)
-            echo "Unknown option: $1"
-            exit 1
-            ;;
-    esac
-done
-
-echo -e "\033[1;33mUsing config file: $CONFIG_FILE\033[0m"
-
-# 初始化变量，使用 YAML 文件中的默认值
-NUM_GPUS=$(yq -r '.NUM_GPUS' $CONFIG_FILE)
-MODEL=$(yq -r '.MODEL' $CONFIG_FILE)
-NAME=$(yq -r '.NAME' $CONFIG_FILE)
-
-echo -e "\033[1;33mModel: $NAME\033[0m"
-
-DATA_PATH=$(yq -r '.DATA_PATH' $CONFIG_FILE)
-OUTPUT_DIR=$(yq -r '.OUTPUT_DIR' $CONFIG_FILE)
-BATCH_SIZE=$(yq -r '.BATCH_SIZE' $CONFIG_FILE)
-ACCUM=$(yq -r '.ACCUM' $CONFIG_FILE)
-EPOCHS=$(yq -r '.EPOCHS' $CONFIG_FILE)
-WARMUP_EPOCHS=$(yq -r '.WARMUP_EPOCHS' $CONFIG_FILE)
-BASE_LR=$(yq -r '.BASE_LR' $CONFIG_FILE)
-INPUT_SIZE=$(yq -r '.INPUT_SIZE' $CONFIG_FILE)
-MASK_RATIO=$(yq -r '.MASK_RATIO' $CONFIG_FILE)
-
-# 获取当前日期和时间
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-
-# 动态生成日志目录
-LOG_DIR="./logs/mae_train/tb_${CURRENT_DATETIME}"
-
-# 运行训练脚本
-python main_pretrain.py \
-    --model ${MODEL} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --accum_iter ${ACCUM} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --input_size ${INPUT_SIZE} \
-    --mask_ratio ${MASK_RATIO} \
-    --norm_pix_loss \
-    --log_dir ${LOG_DIR}
\ No newline at end of file
diff --git a/bash_scripts_discard/mae_train_deit.sh b/bash_scripts_discard/mae_train_deit.sh
deleted file mode 100644
index 3a1ba3c..0000000
--- a/bash_scripts_discard/mae_train_deit.sh
+++ /dev/null
@@ -1,35 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-NAME="mae_train_deit"
-NUM_GPUS=6
-MODEL="mae_deit_tiny_patch4"
-DATA_PATH="./dataset/cifar10_dataset"
-OUTPUT_DIR="./ckpts/${NAME}/pretrained"
-BATCH_SIZE=64
-ACCUM=2
-EPOCHS=200
-WARMUP_EPOCHS=10
-BASE_LR=1.5e-4
-INPUT_SIZE=32
-MASK_RATIO=0.75
-
-# 获取当前日期和时间
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-
-# 动态生成日志目录
-LOG_DIR="./logs/${NAME}/tb_${NAME}_${CURRENT_DATETIME}"
-
-python main_pretrain.py \
-    --model ${MODEL} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --accum_iter ${ACCUM} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --input_size ${INPUT_SIZE} \
-    --mask_ratio ${MASK_RATIO} \
-    --norm_pix_loss \
-    --log_dir ${LOG_DIR}
\ No newline at end of file
diff --git a/bash_scripts_discard/mae_train_deit_nGPUs.sh b/bash_scripts_discard/mae_train_deit_nGPUs.sh
deleted file mode 100644
index 010b62b..0000000
--- a/bash_scripts_discard/mae_train_deit_nGPUs.sh
+++ /dev/null
@@ -1,34 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-NUM_GPUS=3
-NAME="mae_deit_pretrain"
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-LOG_DIR="./logs/${NAME}/tb_${CURRENT_DATETIME}"
-MODEL="mae_deit_tiny_patch4"
-DATA_PATH="./dataset/cifar10_dataset"
-OUTPUT_DIR="./ckpts/${NAME}/${CURRENT_DATETIME}"
-BATCH_SIZE=256
-ACCUM=2
-EPOCHS=200
-WARMUP_EPOCHS=10
-BASE_LR=1.5e-4
-INPUT_SIZE=32
-MASK_RATIO=0.75
-
-torchrun --nproc_per_node=${NUM_GPUS} main_pretrain.py \
-    --world_size ${NUM_GPUS} \
-    --name ${NAME} \
-    --model ${MODEL} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --accum_iter ${ACCUM} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --input_size ${INPUT_SIZE} \
-    --mask_ratio ${MASK_RATIO} \
-    --norm_pix_loss \
-    --log_dir ${LOG_DIR} \
-    --current_datetime ${CURRENT_DATETIME} \
\ No newline at end of file
diff --git a/bash_scripts_discard/mae_train_vit.sh b/bash_scripts_discard/mae_train_vit.sh
deleted file mode 100644
index e8ece6d..0000000
--- a/bash_scripts_discard/mae_train_vit.sh
+++ /dev/null
@@ -1,36 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-NAME="mae_train_vit"
-NUM_GPUS=2
-MODEL="mae_vit_tiny_patch4"
-DATA_PATH="./dataset/cifar10_dataset"
-OUTPUT_DIR="./ckpts/${NAME}/pretrained"
-BATCH_SIZE=64
-ACCUM=2
-EPOCHS=200
-WARMUP_EPOCHS=10
-BASE_LR=1.5e-4
-INPUT_SIZE=32
-MASK_RATIO=0.75
-
-# 获取当前日期和时间
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-
-# 动态生成日志目录
-LOG_DIR="./logs/${NAME}/tb_${NAME}_${CURRENT_DATETIME}"
-
-python main_pretrain.py \
-    --model ${MODEL} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --accum_iter ${ACCUM} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --input_size ${INPUT_SIZE} \
-    --mask_ratio ${MASK_RATIO} \
-    --norm_pix_loss \
-    --log_dir ${LOG_DIR}\
-    --ckpt_name ${NAME}
\ No newline at end of file
diff --git a/bash_scripts_nGPUs/Bmae_train.sh b/bash_scripts_nGPUs/Bmae_train.sh
deleted file mode 100644
index 7cb3cf5..0000000
--- a/bash_scripts_nGPUs/Bmae_train.sh
+++ /dev/null
@@ -1,147 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-# 默认值设置
-NUM_GPUS=3
-NAME="Bmae_deit_pretrain"
-MODEL="mae_deit_tiny_patch4"
-DATA_PATH="./dataset/cifar10_dataset"
-BATCH_SIZE=256
-ACCUM=2
-EPOCHS=200
-WARMUP_EPOCHS=10
-BASE_LR=1.5e-4
-INPUT_SIZE=32
-MASK_RATIO=0.75
-BOOTSTRAP_STEPS=5
-BOOTSTRAP_METHOD='Last_layer'
-EMA_DECAY=0.99
-DEVICE="cuda:0"
-USE_EMA=false  # 默认不使用 EMA
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-
-# 解析命令行参数
-while [[ $# -gt 0 ]]; do
-  case $1 in
-    --name)
-      NAME="$2"
-      shift 2
-      ;;
-    --model)
-      MODEL="$2"
-      shift 2
-      ;;
-    --data_path)
-      DATA_PATH="$2"
-      shift 2
-      ;;
-    --batch_size)
-      BATCH_SIZE="$2"
-      shift 2
-      ;;
-    --accum_iter)
-      ACCUM="$2"
-      shift 2
-      ;;
-    --epochs)
-      EPOCHS="$2"
-      shift 2
-      ;;
-    --warmup_epochs)
-      WARMUP_EPOCHS="$2"
-      shift 2
-      ;;
-    --blr)
-      BASE_LR="$2"
-      shift 2
-      ;;
-    --input_size)
-      INPUT_SIZE="$2"
-      shift 2
-      ;;
-    --mask_ratio)
-      MASK_RATIO="$2"
-      shift 2
-      ;;
-    --bootstrap_steps)
-      BOOTSTRAP_STEPS="$2"
-      shift 2
-      ;;
-    --bootstrap_method)
-      BOOTSTRAP_METHOD="$2"
-      shift 2
-      ;;
-    --ema_decay)
-      EMA_DECAY="$2"
-      shift 2
-      ;;
-    --device)
-      DEVICE="$2"
-      shift 2
-      ;;
-    --use_ema)
-      USE_EMA=true  # 如果传入 --use_ema，则启用 EMA
-      shift 1
-      ;;
-    --current_datetime)
-      CURRENT_DATETIME="$2"
-      shift 2
-      ;;
-    *)
-      echo "Unknown argument: $1"
-      exit 1
-      ;;
-  esac
-done
-
-# 动态生成日志和输出目录
-LOG_DIR="./logs/${NAME}/tb_${CURRENT_DATETIME}"
-OUTPUT_DIR="./ckpts/${NAME}/${CURRENT_DATETIME}"
-
-# 创建日志目录
-mkdir -p ${LOG_DIR}
-
-# 将参数写入 YAML 格式的参数表文件
-PARAMS_FILE="${LOG_DIR}/params.yaml"
-echo "name: ${NAME}" > ${PARAMS_FILE}
-echo "model: ${MODEL}" >> ${PARAMS_FILE}
-echo "data_path: ${DATA_PATH}" >> ${PARAMS_FILE}
-echo "batch_size: ${BATCH_SIZE}" >> ${PARAMS_FILE}
-echo "accum_iter: ${ACCUM}" >> ${PARAMS_FILE}
-echo "epochs: ${EPOCHS}" >> ${PARAMS_FILE}
-echo "warmup_epochs: ${WARMUP_EPOCHS}" >> ${PARAMS_FILE}
-echo "base_lr: ${BASE_LR}" >> ${PARAMS_FILE}
-echo "input_size: ${INPUT_SIZE}" >> ${PARAMS_FILE}
-echo "mask_ratio: ${MASK_RATIO}" >> ${PARAMS_FILE}
-echo "bootstrap_steps: ${BOOTSTRAP_STEPS}" >> ${PARAMS_FILE}
-echo "bootstrap_method: ${BOOTSTRAP_METHOD}" >> ${PARAMS_FILE}
-echo "ema_decay: ${EMA_DECAY}" >> ${PARAMS_FILE}
-echo "device: ${DEVICE}" >> ${PARAMS_FILE}
-echo "use_ema: ${USE_EMA}" >> ${PARAMS_FILE}
-echo "log_dir: ${LOG_DIR}" >> ${PARAMS_FILE}
-echo "output_dir: ${OUTPUT_DIR}" >> ${PARAMS_FILE}
-echo "current_datetime: ${CURRENT_DATETIME}" >> ${PARAMS_FILE}
-
-# 执行 Python 脚本
-torchrun --nproc_per_node=${NUM_GPUS} main_pretrain.py \
-    --world_size ${NUM_GPUS} \
-    --name ${NAME} \
-    --model ${MODEL} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --accum_iter ${ACCUM} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --input_size ${INPUT_SIZE} \
-    --mask_ratio ${MASK_RATIO} \
-    --norm_pix_loss \
-    --log_dir ${LOG_DIR} \
-    --is_bootstrapping \
-    --bootstrap_steps ${BOOTSTRAP_STEPS} \
-    --bootstrap_method ${BOOTSTRAP_METHOD} \
-    $( [ "${USE_EMA}" = true ] && echo "--use_ema" ) \
-    --ema_decay ${EMA_DECAY} \
-    --device ${DEVICE} \
-    --current_datetime ${CURRENT_DATETIME}
\ No newline at end of file
diff --git a/bash_scripts_nGPUs/mae_train_nGPUs.sh b/bash_scripts_nGPUs/mae_train_nGPUs.sh
deleted file mode 100644
index 5217180..0000000
--- a/bash_scripts_nGPUs/mae_train_nGPUs.sh
+++ /dev/null
@@ -1,34 +0,0 @@
-#!/bin/bash
-export OMP_NUM_THREADS=4
-
-NUM_GPUS=3
-NAME="mae_deit_pretrain_ngpu"
-CURRENT_DATETIME=$(date +"%Y-%m-%d_%H-%M-%S")
-LOG_DIR="./logs/${NAME}/tb_${CURRENT_DATETIME}"
-MODEL="mae_deit_tiny_patch4"
-DATA_PATH="./dataset/cifar10_dataset"
-OUTPUT_DIR="./ckpts/${NAME}/${CURRENT_DATETIME}"
-BATCH_SIZE=256
-ACCUM=2
-EPOCHS=200
-WARMUP_EPOCHS=10
-BASE_LR=1.5e-4
-INPUT_SIZE=32
-MASK_RATIO=0.75
-
-torchrun --nproc_per_node=${NUM_GPUS} main_pretrain.py \
-    --world_size ${NUM_GPUS} \
-    --name ${NAME} \
-    --model ${MODEL} \
-    --data_path ${DATA_PATH} \
-    --output_dir ${OUTPUT_DIR} \
-    --batch_size ${BATCH_SIZE} \
-    --accum_iter ${ACCUM} \
-    --epochs ${EPOCHS} \
-    --warmup_epochs ${WARMUP_EPOCHS} \
-    --blr ${BASE_LR} \
-    --input_size ${INPUT_SIZE} \
-    --mask_ratio ${MASK_RATIO} \
-    --norm_pix_loss \
-    --log_dir ${LOG_DIR} \
-    --current_datetime ${CURRENT_DATETIME} \
\ No newline at end of file
diff --git a/dataset/cal_mean_std.py b/dataset/cal_mean_std.py
deleted file mode 100644
index 5a874fd..0000000
--- a/dataset/cal_mean_std.py
+++ /dev/null
@@ -1,21 +0,0 @@
-from torchvision import datasets, transforms
-from torch.utils.data import DataLoader
-import numpy as np
-
-transform = transforms.ToTensor()
-dataset = datasets.ImageFolder('./dataset/cifar10_dataset', transform=transform)
-loader = DataLoader(dataset, batch_size=64, shuffle=False)
-
-mean = 0.0
-std = 0.0
-total_samples = 0
-for images, _ in loader:
-    batch_samples = images.size(0)  # batch size (number of images in this batch)
-    images = images.view(batch_samples, images.size(1), -1)  # Flatten height and width
-    mean += images.mean(2).sum(0)
-    std += images.std(2).sum(0)
-    total_samples += batch_samples
-
-mean /= total_samples
-std /= total_samples
-print(f"Mean: {mean}, Std: {std}")
\ No newline at end of file
diff --git a/dataset/get_cifar10.py b/dataset/get_cifar10.py
deleted file mode 100644
index 46abbc0..0000000
--- a/dataset/get_cifar10.py
+++ /dev/null
@@ -1,129 +0,0 @@
-import os
-import torch
-from datasets import load_dataset
-from PIL import Image
-from tqdm import tqdm
-import multiprocessing
-from functools import partial
-
-def save_image(example, split, classes, output_directory):
-    """
-    保存单张图片到对应的目录
-
-    参数:
-        example (dict): 数据集中的单个样本
-        split (str): 数据集划分 ('train' 或 'test')
-        classes (list): 类别名称列表
-        output_directory (str): 输出的根目录
-
-    返回:
-        str: 保存的图片路径
-    """
-    # 获取图片和标签
-    img = example['img']
-    label = example['label']
-    class_name = classes[label]
-    
-    # 确定类别对应的输出目录
-    class_output_dir = os.path.join(output_directory, class_name)
-    
-    # 如果目录不存在，则创建
-    os.makedirs(class_output_dir, exist_ok=True)
-    
-    # 生成唯一的文件名
-    img_path = os.path.join(class_output_dir, f'{class_name}_{split}_{example["idx"]}.png')
-    
-    # 保存图片
-    img.save(img_path)
-    
-    return img_path
-
-def download_and_prepare_cifar10(root_dir, num_workers=None):
-    """
-    下载 CIFAR-10 数据集并保存为 ImageFolder 格式
-
-    参数:
-        root_dir (str): 保存数据集的根目录
-        num_workers (int, optional): 多进程的工作线程数，默认为 None（使用所有可用核心）
-    """
-    # 创建训练集和测试集的目录
-    train_dir = os.path.join(root_dir, 'train')
-    test_dir = os.path.join(root_dir, 'val')
-    os.makedirs(train_dir, exist_ok=True)
-    os.makedirs(test_dir, exist_ok=True)
-
-    # CIFAR-10 的类别名称
-    classes = [
-        'airplane', 'automobile', 'bird', 'cat', 'deer', 
-        'dog', 'frog', 'horse', 'ship', 'truck'
-    ]
-
-    # 从 Hugging Face 下载数据集
-    dataset = load_dataset('uoft-cs/cifar10', num_proc=24)
-
-    # 为数据集添加索引列，用于生成唯一文件名
-    for split in ['train', 'test']:
-        dataset[split] = dataset[split].add_column('idx', range(len(dataset[split])))
-
-    # 定义处理数据集划分的函数
-    def process_split(split):
-        """
-        处理指定的数据集划分（训练集或测试集）
-
-        参数:
-            split (str): 数据集划分 ('train' 或 'test')
-        """
-        # 确定输出目录
-        output_directory = train_dir if split == 'train' else test_dir
-        
-        # 使用 partial 函数绑定参数
-        save_func = partial(
-            save_image, 
-            split=split, 
-            classes=classes, 
-            output_directory=output_directory
-        )
-        
-        # 使用多进程保存图片
-        with multiprocessing.Pool(processes=num_workers) as pool:
-            # 使用 tqdm 显示进度条
-            list(tqdm(
-                pool.imap(save_func, dataset[split]), 
-                total=len(dataset[split]), 
-                desc=f'保存 {split} 图片中'
-            ))
-
-    # 保存训练集和测试集图片
-    process_split('train')
-    process_split('test')
-
-    print(f"CIFAR-10 数据集已下载并保存到 {root_dir}")
-
-# 示例用法
-if __name__ == "__main__":
-    # 指定保存数据集的根目录
-    root_dir = './dataset/cifar10_dataset'
-    
-    # 下载并准备数据集
-    # 可以指定工作线程数，或者设置为 None 使用所有核心
-    download_and_prepare_cifar10(root_dir, num_workers=None)
-
-    # 验证数据集结构
-    import torchvision.datasets as datasets
-    import torchvision.transforms as transforms
-
-    # 定义简单的变换（可选）
-    transform = transforms.Compose([
-        transforms.ToTensor(),
-        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
-    ])
-
-    # 使用 ImageFolder 加载数据集
-    dataset_train = datasets.ImageFolder(
-        os.path.join(root_dir, 'train'), 
-        transform=transform
-    )
-
-    # 打印数据集信息
-    print(f"训练集总图片数: {len(dataset_train)}")
-    print(f"类别: {dataset_train.classes}")
\ No newline at end of file
diff --git a/engine_pretrain.py b/engine_pretrain.py
index 20ab7c3..4ea0d13 100644
--- a/engine_pretrain.py
+++ b/engine_pretrain.py
@@ -22,11 +22,7 @@ def train_one_epoch(model: torch.nn.Module,
                     data_loader: Iterable, optimizer: torch.optim.Optimizer,
                     device: torch.device, epoch: int, loss_scaler,
                     log_writer=None,
-                    args=None,
-                    last_model=None,
-                    method_class=None,
-                    optimizer_method_class=None,
-                    ):
+                    args=None):
     model.train(True)
     metric_logger = misc.MetricLogger(delimiter="  ")
     metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))
@@ -36,8 +32,6 @@ def train_one_epoch(model: torch.nn.Module,
     accum_iter = args.accum_iter
 
     optimizer.zero_grad()
-    if method_class is not None and last_model is not None and optimizer_method_class is not None:
-        optimizer_method_class.zero_grad()
 
     if log_writer is not None:
         print('log_dir: {}'.format(log_writer.log_dir))
@@ -47,13 +41,11 @@ def train_one_epoch(model: torch.nn.Module,
         # we use a per iteration (instead of per epoch) lr scheduler
         if data_iter_step % accum_iter == 0:
             lr_sched.adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)
-            if method_class is not None and last_model is not None and optimizer_method_class is not None:
-                lr_sched.adjust_learning_rate(optimizer_method_class, data_iter_step / len(data_loader) + epoch, args)
 
         samples = samples.to(device, non_blocking=True)
 
         with torch.cuda.amp.autocast():
-            loss, _, _ = model(samples, mask_ratio=args.mask_ratio, last_model=last_model, method_class=method_class)
+            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)
 
         loss_value = loss.item()
 
@@ -62,28 +54,9 @@ def train_one_epoch(model: torch.nn.Module,
             sys.exit(1)
 
         loss /= accum_iter
-        
-        # 更新 method_class 的参数
-        if method_class is not None and last_model is not None and optimizer_method_class is not None:
-            loss_scaler(loss, optimizer, parameters=list(model.parameters()),
-                        update_grad=(data_iter_step + 1) % accum_iter == 0)
-            optimizer_method_class.step()
-
-            # print("Gradients of method_class parameters:")
-            # for name, param in method_class.named_parameters():
-            #     # print(param.requires_grad)
-            #     if param.grad is not None:
-            #         print(f"{name}: {param.grad}")
-            #     else:
-            #         print(f"{name}: No gradient")
-
-        else:
-            loss_scaler(loss, optimizer, parameters=list(model.parameters()),
-                        update_grad=(data_iter_step + 1) % accum_iter == 0)
-                
+        loss_scaler(loss, optimizer, parameters=model.parameters(),
+                    update_grad=(data_iter_step + 1) % accum_iter == 0)
         if (data_iter_step + 1) % accum_iter == 0:
-            if method_class is not None and last_model is not None and optimizer_method_class is not None:
-                optimizer_method_class.zero_grad()
             optimizer.zero_grad()
 
         torch.cuda.synchronize()
diff --git a/experiments/finetune_all_bs_step_tuning.py b/experiments/finetune_all_bs_step_tuning.py
deleted file mode 100644
index 64f8c05..0000000
--- a/experiments/finetune_all_bs_step_tuning.py
+++ /dev/null
@@ -1,126 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_1_use_ema_False/2025-05-02_00-22-32/Bmae-1-199.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_1_use_ema_True/2025-05-01_23-10-49/Bmae-ema-1-199.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_5_use_ema_False/2025-05-02_03-07-33/Bmae-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_5_use_ema_True/2025-05-02_01-33-55/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_10_use_ema_False/2025-05-02_06-17-41/Bmae-10-19.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_10_use_ema_True/2025-05-02_04-41-10/Bmae-ema-10-19.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_100_use_ema_False/2025-05-02_09-32-13/Bmae-100-1.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_100_use_ema_True/2025-05-02_07-53-50/Bmae-ema-100-1.pth",
-        ]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/bootstrap_steps_finetune.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture metrics
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "bootstrap_steps_" + ckpt.split('/')[1].split('_bootstrap_steps_')[1].split('/')[0]
-    name = f"Bmae_deit_finetune_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_finetune.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:0",  # finetune必须是cuda:0
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: CKPT={ckpt}")
-        return None
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# Logging results
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging finetune bs_steps tuning, at {log_current_datetime}\n")
-    print(f"Start logging finetune bs_steps tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging finetune bs_step tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging finetune bs_step tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/finetune_all_half_life_tuning.py b/experiments/finetune_all_half_life_tuning.py
deleted file mode 100644
index 7ecf54e..0000000
--- a/experiments/finetune_all_half_life_tuning.py
+++ /dev/null
@@ -1,120 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_pretrain_ema_decay_0.999/2025-04-30_11-34-13/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_ema_decay_0.99/2025-04-30_13-07-23/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_ema_decay_0.9/2025-04-30_14-39-30/Bmae-ema-5-39.pth",
-        ]
-# Output log file
-log_file = "./experiments/hyperparam_results/half_life_finetune.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture metrics
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "ema_decay_" + ckpt.split('/')[1].split('_ema_decay_')[1].split('/')[0]
-    name = f"Bmae_deit_finetune_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_finetune.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:0",  # finetune必须是cuda:0
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: CKPT={ckpt}")
-        return None
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# Logging results
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging finetune half_life tuning, at {log_current_datetime}\n")
-    print(f"Start logging finetune half_life tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging finetune half_life tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging finetune half_life tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/finetune_all_lr_tuning.py b/experiments/finetune_all_lr_tuning.py
deleted file mode 100644
index db06fae..0000000
--- a/experiments/finetune_all_lr_tuning.py
+++ /dev/null
@@ -1,125 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_1_warmup_10_base_lr_0.0005/2025-05-02_08-33-33/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_2_warmup_10_base_lr_0.001/2025-05-02_02-24-13/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_2_warmup_10_base_lr_0.0001/2025-05-01_23-19-33/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_2_warmup_10_base_lr_0.0005/2025-05-02_10-09-24/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_2_warmup_15_base_lr_0.0005/2025-05-02_05-28-59/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_2_warmup_20_base_lr_0.0005/2025-05-02_07-01-17/Bmae-ema-5-39.pth",
-        ""
-        ]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/lr_finetune.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture metrics
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "accum_" + ckpt.split('/')[1].split('_accum_')[1].split('/')[0]
-    name = f"Bmae_deit_finetune_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_finetune.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:0",  # finetune必须是cuda:0
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: CKPT={ckpt}")
-        return None
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# Logging results
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging finetune lr tuning, at {log_current_datetime}\n")
-    print(f"Start logging finetune lr tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging finetune lr tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging finetune lr tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/finetune_all_method_tuning.py b/experiments/finetune_all_method_tuning.py
deleted file mode 100644
index 5914d91..0000000
--- a/experiments/finetune_all_method_tuning.py
+++ /dev/null
@@ -1,124 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Adaptive_layer_fusion/2025-05-02_00-52-28/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Cross_layer_cross_attention/2025-05-02_07-45-10/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Cross_layer_fusion/2025-05-02_02-33-52/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Cross_layer_self_attention/2025-05-02_05-58-03/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Fixed_layer_fusion/2025-05-01_23-11-53/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Gated_fusion_dynamic/2025-05-02_04-15-56/Bmae-ema-200-0.pth",
-        ]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/method_finetune.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture metrics
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "method_" + ckpt.split('/')[1].split('_method_')[1].split('/')[0]
-    name = f"Bmae_deit_finetune_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_finetune.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:0",  # finetune必须是cuda:0
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: CKPT={ckpt}")
-        return None
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# Logging results
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging finetune method tuning, at {log_current_datetime}\n")
-    print(f"Start logging finetune method tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging finetune method tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging finetune method tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/finetune_all_norm_tuning.py b/experiments/finetune_all_norm_tuning.py
deleted file mode 100644
index ffc8ff6..0000000
--- a/experiments/finetune_all_norm_tuning.py
+++ /dev/null
@@ -1,120 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_pretrain_norm_pix_loss_False/2025-05-02_00-46-09/Bmae-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_norm_pix_loss_True/2025-05-01_23-13-28/Bmae-5-39.pth",
-        ]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/norm_finetune.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture metrics
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "norm_" + ckpt.split('/')[1].split('_norm_')[1].split('/')[0]
-    name = f"Bmae_deit_finetune_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_finetune.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:0",  # finetune必须是cuda:0
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: CKPT={ckpt}")
-        return None
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# Logging results
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging finetune norm tuning, at {log_current_datetime}\n")
-    print(f"Start logging finetune norm tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging finetune norm tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging finetune norm tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/finetune_all_optim_tuning.py b/experiments/finetune_all_optim_tuning.py
deleted file mode 100644
index 0efde8c..0000000
--- a/experiments/finetune_all_optim_tuning.py
+++ /dev/null
@@ -1,121 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_pretrain_optim_AdamW/2025-05-02_05-33-17/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_optim_RMSprop/2025-05-02_02-18-48/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_optim_SGD/2025-05-02_03-55-50/Bmae-ema-200-0.pth",
-        ]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/optim_finetune.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture metrics
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "optim_" + ckpt.split('/')[1].split('_optim_')[1].split('/')[0]
-    name = f"Bmae_deit_finetune_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_finetune.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:0",  # finetune必须是cuda:0
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: CKPT={ckpt}")
-        return None
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# Logging results
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging finetune optim tuning, at {log_current_datetime}\n")
-    print(f"Start logging finetune optim tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging finetune optim tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging finetune optim tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/linprobe_all_bs_step_tuning.py b/experiments/linprobe_all_bs_step_tuning.py
deleted file mode 100644
index 89281ee..0000000
--- a/experiments/linprobe_all_bs_step_tuning.py
+++ /dev/null
@@ -1,135 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_1_use_ema_False/2025-05-02_00-22-32/Bmae-1-199.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_1_use_ema_True/2025-05-01_23-10-49/Bmae-ema-1-199.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_5_use_ema_False/2025-05-02_03-07-33/Bmae-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_5_use_ema_True/2025-05-02_01-33-55/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_10_use_ema_False/2025-05-02_06-17-41/Bmae-10-19.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_10_use_ema_True/2025-05-02_04-41-10/Bmae-ema-10-19.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_100_use_ema_False/2025-05-02_09-32-13/Bmae-100-1.pth",
-        "ckpts/Bmae_deit_pretrain_bootstrap_steps_100_use_ema_True/2025-05-02_07-53-50/Bmae-ema-100-1.pth",
-        ]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/bootstrap_steps_linprobe.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "bootstrap_steps_" + ckpt.split('/')[1].split('_bootstrap_steps_')[1].split('/')[0]
-    # print(ckpt_name)
-    # exit(0)
-    name = f"Bmae_deit_linprobe_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_linear.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:2",
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: KPT={ckpt}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging linprobe bs_steps tuning, at {log_current_datetime}\n")
-    print(f"Start logging linprobe bs_steps tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging linprobe bs_step tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging linprobe bs_step tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/linprobe_all_half_life_tuning.py b/experiments/linprobe_all_half_life_tuning.py
deleted file mode 100644
index c56a9b5..0000000
--- a/experiments/linprobe_all_half_life_tuning.py
+++ /dev/null
@@ -1,130 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_pretrain_ema_decay_0.999/2025-04-30_11-34-13/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_ema_decay_0.99/2025-04-30_13-07-23/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_ema_decay_0.9/2025-04-30_14-39-30/Bmae-ema-5-39.pth",
-        ]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/half_life_linprobe.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "ema_decay_" + ckpt.split('/')[1].split('_ema_decay_')[1].split('/')[0]
-    # print(ckpt_name)
-    # exit(0)
-    name = f"Bmae_deit_linprobe_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_linear.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:2",
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: KPT={ckpt}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging linprobe half_life tuning, at {log_current_datetime}\n")
-    print(f"Start logging linprobe half_life tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging linprobe half_life tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging linprobe half_life tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/linprobe_all_lr_tuning.py b/experiments/linprobe_all_lr_tuning.py
deleted file mode 100644
index 4b97d05..0000000
--- a/experiments/linprobe_all_lr_tuning.py
+++ /dev/null
@@ -1,135 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_1_warmup_10_base_lr_0.0005/2025-05-02_08-33-33/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_2_warmup_10_base_lr_0.001/2025-05-02_02-24-13/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_2_warmup_10_base_lr_0.0001/2025-05-01_23-19-33/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_2_warmup_10_base_lr_0.0005/2025-05-02_10-09-24/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_2_warmup_15_base_lr_0.0005/2025-05-02_05-28-59/Bmae-ema-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_accum_2_warmup_20_base_lr_0.0005/2025-05-02_07-01-17/Bmae-ema-5-39.pth",
-        ""
-        ]
-
-
-# Output log file
-log_file = "./experiments/hyperparam_results/lr_linprobe.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "accum_" + ckpt.split('/')[1].split('_accum_')[1].split('/')[0]
-    # print(ckpt_name)
-    # exit(0)
-    name = f"Bmae_deit_linprobe_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_linear.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:0",
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: KPT={ckpt}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging linprobe lr tuning, at {log_current_datetime}\n")
-    print(f"Start logging linprobe lr tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging linprobe lr tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging linprobe lr tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/linprobe_all_method_tuning.py b/experiments/linprobe_all_method_tuning.py
deleted file mode 100644
index 3bee5d9..0000000
--- a/experiments/linprobe_all_method_tuning.py
+++ /dev/null
@@ -1,133 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Adaptive_layer_fusion/2025-05-02_00-52-28/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Cross_layer_cross_attention/2025-05-02_07-45-10/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Cross_layer_fusion/2025-05-02_02-33-52/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Cross_layer_self_attention/2025-05-02_05-58-03/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Fixed_layer_fusion/2025-05-01_23-11-53/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_method_Gated_fusion_dynamic/2025-05-02_04-15-56/Bmae-ema-200-0.pth",
-        ]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/method_linprobe.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "method_" + ckpt.split('/')[1].split('_method_')[1].split('/')[0]
-    # print(ckpt_name)
-    # exit(0)
-    name = f"Bmae_deit_linprobe_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_linear.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:1",
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: KPT={ckpt}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging linprobe method tuning, at {log_current_datetime}\n")
-    print(f"Start logging linprobe method tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging linprobe method tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging linprobe method tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/linprobe_all_norm_tuning.py b/experiments/linprobe_all_norm_tuning.py
deleted file mode 100644
index 0720eee..0000000
--- a/experiments/linprobe_all_norm_tuning.py
+++ /dev/null
@@ -1,129 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_pretrain_norm_pix_loss_False/2025-05-02_00-46-09/Bmae-5-39.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_norm_pix_loss_True/2025-05-01_23-13-28/Bmae-5-39.pth",
-        ]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/norm_linprobe.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "norm_" + ckpt.split('/')[1].split('_norm_')[1].split('/')[0]
-    # print(ckpt_name)
-    # exit(0)
-    name = f"Bmae_deit_linprobe_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_linear.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:1",
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: KPT={ckpt}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging linprobe norm tuning, at {log_current_datetime}\n")
-    print(f"Start logging linprobe norm tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging linprobe norm tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging linprobe norm tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/linprobe_all_optim_tuning.py b/experiments/linprobe_all_optim_tuning.py
deleted file mode 100644
index a8d78a1..0000000
--- a/experiments/linprobe_all_optim_tuning.py
+++ /dev/null
@@ -1,130 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-CKPTS = [
-        "ckpts/Bmae_deit_pretrain_pretrain_optim_AdamW/2025-05-02_05-33-17/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_optim_RMSprop/2025-05-02_02-18-48/Bmae-ema-200-0.pth",
-        "ckpts/Bmae_deit_pretrain_pretrain_optim_SGD/2025-05-02_03-55-50/Bmae-ema-200-0.pth",
-        ]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/optim_linprobe.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to parse the log file and extract metrics
-def parse_log_file(log_path):
-    max_test_acc1 = float('-inf')
-    max_test_acc5 = float('-inf')
-    min_train_loss = float('inf')
-    min_test_loss = float('inf')
-
-    try:
-        with open(log_path, "r") as log_file:
-            lines = log_file.readlines()
-            for line in lines:
-                try:
-                    # 解析每一行 JSON 数据
-                    entry = json.loads(line)
-                    # 更新最大 test_acc1 和 test_acc5
-                    max_test_acc1 = max(max_test_acc1, entry.get("test_acc1", float('-inf')))
-                    max_test_acc5 = max(max_test_acc5, entry.get("test_acc5", float('-inf')))
-                    # 更新最小 train_loss 和 test_loss
-                    min_train_loss = min(min_train_loss, entry.get("train_loss", float('inf')))
-                    min_test_loss = min(min_test_loss, entry.get("test_loss", float('inf')))
-                except json.JSONDecodeError:
-                    print(f"Error parsing line in {log_path}: {line.strip()}")
-    except FileNotFoundError:
-        print(f"Log file not found: {log_path}")
-        return None
-
-    return {
-        "max_test_acc1": max_test_acc1,
-        "max_test_acc5": max_test_acc5,
-        "min_train_loss": min_train_loss,
-        "min_test_loss": min_test_loss,
-    }
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(ckpt):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    ckpt_name = "optim_" + ckpt.split('/')[1].split('_optim_')[1].split('/')[0]
-    # print(ckpt_name)
-    # exit(0)
-    name = f"Bmae_deit_linprobe_{ckpt_name}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_eval_linear.sh", 
-        "--finetune", str(ckpt),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:1",
-        "--save_frequency", "200"  # 相当于不save checkpoint
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: KPT={ckpt}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    # Read and parse the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    stats = parse_log_file(log_path)
-    if stats is not None:
-        print(f"Stats for {ckpt}: {stats}")
-        return stats
-    else:
-        print(f"Failed to parse stats for {ckpt}")
-        return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging linprobe optim tuning, at {log_current_datetime}\n")
-    print(f"Start logging linprobe optim tuning, at {log_current_datetime}")
-
-    for ckpt in CKPTS:
-        print(f"Running training with CKPT={ckpt}")
-        
-        stats = run_training(ckpt)
-        
-        if stats is not None:
-            log.write(f"CKPT={ckpt}, MAX_TEST_ACC1={stats['max_test_acc1']}, MAX_TEST_ACC5={stats['max_test_acc5']}, "
-                      f"MIN_TRAIN_LOSS={stats['min_train_loss']}, MIN_TEST_LOSS={stats['min_test_loss']}\n")
-            print(f"CKPT={ckpt}. Stats: {stats}")
-        else:
-            log.write(f"ERROR: CKPT={ckpt}, STATUS=FAILED\n")
-            print(f"Error with: CKPT={ckpt}. Marking as FAILED.")
-
-    log.write(f"Finish logging linprobe optim tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging linprobe optim tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/pretrain_accum_tuning.py b/experiments/pretrain_accum_tuning.py
deleted file mode 100644
index f63fada..0000000
--- a/experiments/pretrain_accum_tuning.py
+++ /dev/null
@@ -1,102 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-# ACCUM_VALUES = [1, 2, 4]
-ACCUM_VALUES = [4]
-WARMUP_EPOCHS_VALUES = [10]
-BASE_LR_VALUES = [5e-4]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/pretrain_lr_tuning.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(accum, warmup_epochs, base_lr):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    name = f"Bmae_deit_pretrain_pretrain_accum_{accum}_warmup_{warmup_epochs}_base_lr_{base_lr}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_train.sh", 
-        "--accum_iter", str(accum), 
-        "--warmup_epochs", str(warmup_epochs), 
-        "--blr", str(base_lr), 
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:3",
-        "--use_ema",
-        "--save_frequency", "200"
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    with open(log_path, "r") as log_file:
-        lines = log_file.readlines()
-        if lines:
-            last_line = lines[-1]
-            try:
-                # Parse the JSON line to get the training loss
-                last_entry = json.loads(last_line)
-                train_loss = last_entry.get("train_loss")
-                return train_loss
-            except json.JSONDecodeError:
-                print(f"Error parsing last line of {log_path}")
-                return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging pretrain lr tuning, at {log_current_datetime}\n")
-    print(f"Start logging pretrain lr tuning, at {log_current_datetime}")
-
-    # Iterate over all combinations of hyperparameters
-    for accum in ACCUM_VALUES:
-        for warmup_epochs in WARMUP_EPOCHS_VALUES:
-            for base_lr in BASE_LR_VALUES:
-                print(f"Running training with ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}")
-                
-                # Run training and capture the last line's training loss
-                train_loss = run_training(accum, warmup_epochs, base_lr)
-                
-                if train_loss is not None:
-                    log.write(f"ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}, TRAIN_LOSS={train_loss}\n")
-                    print(f"Finished: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}, TRAIN_LOSS={train_loss}")
-                else:
-                    log.write(f"ERROR: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}, STATUS=FAILED\n")
-                    print(f"Error with: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}. Marking as FAILED.")
-
-    log.write(f"Finish logging pretrain lr tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging pretrain lr tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/pretrain_bootstrap_steps_tuning.py b/experiments/pretrain_bootstrap_steps_tuning.py
deleted file mode 100644
index 6f25c76..0000000
--- a/experiments/pretrain_bootstrap_steps_tuning.py
+++ /dev/null
@@ -1,99 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-BOOTSTRAP_STEPS = [1, 5, 10, 100, 200]
-USE_EMA = [True, False]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/pretrain_bootstrap_steps_tuning.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(bootstrap_steps, use_ema):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    name = f"Bmae_deit_pretrain_bootstrap_steps_{bootstrap_steps}_use_ema_{use_ema}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_train.sh", 
-        "--bootstrap_steps", str(bootstrap_steps),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:2",
-        "--save_frequency", "200"
-    ]
-
-    if use_ema:
-        command.append("--use_ema")
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: BOOTSTRAP_STEPS={bootstrap_steps}, USE_EMA={use_ema}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    with open(log_path, "r") as log_file:
-        lines = log_file.readlines()
-        if lines:
-            last_line = lines[-1]
-            try:
-                # Parse the JSON line to get the training loss
-                last_entry = json.loads(last_line)
-                train_loss = last_entry.get("train_loss")
-                return train_loss
-            except json.JSONDecodeError:
-                print(f"Error parsing last line of {log_path}")
-                return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging pretrain bs_steps tuning, at {log_current_datetime}\n")
-    print(f"Start logging pretrain bs_steps tuning, at {log_current_datetime}")
-
-    for bootstrap_steps in BOOTSTRAP_STEPS:
-        for use_ema in USE_EMA:
-            if bootstrap_steps == 150 and use_ema == True:
-                continue
-            print(f"Running training with BOOTSTRAP_STEPS={bootstrap_steps}, USE_EMA={use_ema}")
-            
-            train_loss = run_training(bootstrap_steps, use_ema)
-            
-            if train_loss is not None:
-                log.write(f"BOOTSTRAP_STEPS={bootstrap_steps}, USE_EMA={use_ema}, TRAIN_LOSS={train_loss}\n")
-                print(f"BOOTSTRAP_STEPS={bootstrap_steps}, USE_EMA={use_ema}. TRAIN_LOSS={train_loss}")
-            else:
-                log.write(f"ERROR: BOOTSTRAP_STEPS={bootstrap_steps}, USE_EMA={use_ema}, STATUS=FAILED\n")
-                print(f"Error with: BOOTSTRAP_STEPS={bootstrap_steps}, USE_EMA={use_ema}. Marking as FAILED.")
-
-    log.write(f"Finish logging pretrain bs_step tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging pretrain bs_step tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/pretrain_half_life_tuning.py b/experiments/pretrain_half_life_tuning.py
deleted file mode 100644
index 8b26b3e..0000000
--- a/experiments/pretrain_half_life_tuning.py
+++ /dev/null
@@ -1,96 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-EMA_DECAY = [0.999, 0.99, 0.9]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/pretrain_half_life_tuning.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(ema_decay):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    name = f"Bmae_deit_pretrain_pretrain_ema_decay_{ema_decay}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_train.sh", 
-        "--ema_decay", str(ema_decay), 
-        "--use_ema", 
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:1",
-        "--save_frequency", "200",
-        "--bootstrap_steps", "200",
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: EMA_DECAY={ema_decay}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    with open(log_path, "r") as log_file:
-        lines = log_file.readlines()
-        if lines:
-            last_line = lines[-1]
-            try:
-                # Parse the JSON line to get the training loss
-                last_entry = json.loads(last_line)
-                train_loss = last_entry.get("train_loss")
-                return train_loss
-            except json.JSONDecodeError:
-                print(f"Error parsing last line of {log_path}")
-                return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging pretrain half-life tuning, at {log_current_datetime}\n")
-    print(f"Start logging pretrain half-life tuning, at {log_current_datetime}")
-
-    # Iterate over all combinations of hyperparameters
-    for ema_decay in EMA_DECAY:
-        print(f"Running training with EMA_DECAY={ema_decay}")
-        
-        # Run training and capture the last line's training loss
-        train_loss = run_training(ema_decay)
-        
-        if train_loss is not None:
-            log.write(f"EMA_DECAY={ema_decay}, TRAIN_LOSS={train_loss}\n")
-            print(f"Finished: EMA_DECAY={ema_decay}, TRAIN_LOSS={train_loss}")
-        else:
-            log.write(f"ERROR: EMA_DECAY={ema_decay}, STATUS=FAILED\n")
-            print(f"Error with: EMA_DECAY={ema_decay}. Marking as FAILED.")
-    
-    log.write(f"Finish logging pretrain half-life tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging pretrain half-life tuning, at {log_current_datetime}")
diff --git a/experiments/pretrain_lr_tuning.py b/experiments/pretrain_lr_tuning.py
deleted file mode 100644
index 5419fb7..0000000
--- a/experiments/pretrain_lr_tuning.py
+++ /dev/null
@@ -1,101 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-ACCUM_VALUES = [2]
-WARMUP_EPOCHS_VALUES = [10]
-BASE_LR_VALUES = [1e-4, 5e-4, 1e-3]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/pretrain_lr_tuning.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(accum, warmup_epochs, base_lr):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    name = f"Bmae_deit_pretrain_pretrain_accum_{accum}_warmup_{warmup_epochs}_base_lr_{base_lr}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_train.sh", 
-        "--accum_iter", str(accum), 
-        "--warmup_epochs", str(warmup_epochs), 
-        "--blr", str(base_lr), 
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:3",
-        "--use_ema",
-        "--save_frequency", "200"
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    with open(log_path, "r") as log_file:
-        lines = log_file.readlines()
-        if lines:
-            last_line = lines[-1]
-            try:
-                # Parse the JSON line to get the training loss
-                last_entry = json.loads(last_line)
-                train_loss = last_entry.get("train_loss")
-                return train_loss
-            except json.JSONDecodeError:
-                print(f"Error parsing last line of {log_path}")
-                return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging pretrain lr tuning, at {log_current_datetime}\n")
-    print(f"Start logging pretrain lr tuning, at {log_current_datetime}")
-
-    # Iterate over all combinations of hyperparameters
-    for accum in ACCUM_VALUES:
-        for warmup_epochs in WARMUP_EPOCHS_VALUES:
-            for base_lr in BASE_LR_VALUES:
-                print(f"Running training with ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}")
-                
-                # Run training and capture the last line's training loss
-                train_loss = run_training(accum, warmup_epochs, base_lr)
-                
-                if train_loss is not None:
-                    log.write(f"ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}, TRAIN_LOSS={train_loss}\n")
-                    print(f"Finished: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}, TRAIN_LOSS={train_loss}")
-                else:
-                    log.write(f"ERROR: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}, STATUS=FAILED\n")
-                    print(f"Error with: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}. Marking as FAILED.")
-
-    log.write(f"Finish logging pretrain lr tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging pretrain lr tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/pretrain_method_tuning.py b/experiments/pretrain_method_tuning.py
deleted file mode 100644
index a6faf2e..0000000
--- a/experiments/pretrain_method_tuning.py
+++ /dev/null
@@ -1,97 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-METHOD = ['Fixed_layer_fusion', 'Adaptive_layer_fusion', 'Cross_layer_fusion', \
-          'Gated_fusion_dynamic', 'Cross_layer_self_attention', 'Cross_layer_cross_attention']
-
-# Output log file
-log_file = "./experiments/hyperparam_results/pretrain_method_tuning.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(method):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    name = f"Bmae_deit_pretrain_pretrain_method_{method}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_train.sh", 
-        "--bootstrap_method", str(method),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:0",
-        "--use_ema",
-        "--save_frequency", "200",
-        "--bootstrap_steps", "200",
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: METHOD={method}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    with open(log_path, "r") as log_file:
-        lines = log_file.readlines()
-        if lines:
-            last_line = lines[-1]
-            try:
-                # Parse the JSON line to get the training loss
-                last_entry = json.loads(last_line)
-                train_loss = last_entry.get("train_loss")
-                return train_loss
-            except json.JSONDecodeError:
-                print(f"Error parsing last line of {log_path}")
-                return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging pretrain method tuning, at {log_current_datetime}\n")
-    print(f"Start logging pretrain method tuning, at {log_current_datetime}")
-
-    # Iterate over all combinations of hyperparameters
-    for method in METHOD:
-        print(f"Running training with METHOD={method}")
-        
-        # Run training and capture the last line's training loss
-        train_loss = run_training(method)
-        
-        if train_loss is not None:
-            log.write(f"METHOD={method}, TRAIN_LOSS={train_loss}\n")
-            print(f"Finished: METHOD={method}, TRAIN_LOSS={train_loss}")
-        else:
-            log.write(f"ERROR: METHOD={method}, STATUS=FAILED\n")
-            print(f"Error with: METHOD={method}. Marking as FAILED.")
-
-    log.write(f"Finish logging pretrain method tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging pretrain method tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/pretrain_norm_tuning.py b/experiments/pretrain_norm_tuning.py
deleted file mode 100644
index fc5d8c6..0000000
--- a/experiments/pretrain_norm_tuning.py
+++ /dev/null
@@ -1,98 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-NORM_PIX_LOSS = [True, False]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/pretrain_norm_pix_loss_tuning.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(norm_pix_loss):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    name = f"Bmae_deit_pretrain_pretrain_norm_pix_loss_{norm_pix_loss}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_train.sh",
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:1",
-        # "--use_ema",
-        "--save_frequency", "200",
-        "--bootstrap_steps", "5",
-    ]
-
-    if norm_pix_loss:
-        command.append("--norm_pix_loss")
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: NORM_PIX_LOSS={norm_pix_loss}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    with open(log_path, "r") as log_file:
-        lines = log_file.readlines()
-        if lines:
-            last_line = lines[-1]
-            try:
-                # Parse the JSON line to get the training loss
-                last_entry = json.loads(last_line)
-                train_loss = last_entry.get("train_loss")
-                return train_loss
-            except json.JSONDecodeError:
-                print(f"Error parsing last line of {log_path}")
-                return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging pretrain norm_pix_loss tuning, at {log_current_datetime}\n")
-    print(f"Start logging pretrain norm_pix_loss tuning, at {log_current_datetime}")
-
-    # Iterate over all combinations of hyperparameters
-    for norm_pix_loss in NORM_PIX_LOSS:
-        print(f"Running training with NORM_PIX_LOSS={norm_pix_loss}")
-        
-        # Run training and capture the last line's training loss
-        train_loss = run_training(norm_pix_loss)
-        
-        if train_loss is not None:
-            log.write(f"NORM_PIX_LOSS={norm_pix_loss}, TRAIN_LOSS={train_loss}\n")
-            print(f"Finished: NORM_PIX_LOSS={norm_pix_loss}, TRAIN_LOSS={train_loss}")
-        else:
-            log.write(f"ERROR: NORM_PIX_LOSS={norm_pix_loss}, STATUS=FAILED\n")
-            print(f"Error with: NORM_PIX_LOSS={norm_pix_loss}. Marking as FAILED.")
-
-    log.write(f"Finish logging pretrain norm_pix_loss tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging pretrain norm_pix_loss tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/pretrain_optim_tuning.py b/experiments/pretrain_optim_tuning.py
deleted file mode 100644
index ce73054..0000000
--- a/experiments/pretrain_optim_tuning.py
+++ /dev/null
@@ -1,96 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-OPTIM = ['RMSprop', 'SGD', 'AdamW']
-
-# Output log file
-log_file = "./experiments/hyperparam_results/pretrain_optim_tuning.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(optim):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    name = f"Bmae_deit_pretrain_pretrain_optim_{optim}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_train.sh", 
-        "--optim", str(optim),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:1",
-        "--use_ema",
-        "--save_frequency", "200",
-        "--bootstrap_steps", "200",
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: OPTIM={optim}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    with open(log_path, "r") as log_file:
-        lines = log_file.readlines()
-        if lines:
-            last_line = lines[-1]
-            try:
-                # Parse the JSON line to get the training loss
-                last_entry = json.loads(last_line)
-                train_loss = last_entry.get("train_loss")
-                return train_loss
-            except json.JSONDecodeError:
-                print(f"Error parsing last line of {log_path}")
-                return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging pretrain optim tuning, at {log_current_datetime}\n")
-    print(f"Start logging pretrain optim tuning, at {log_current_datetime}")
-
-    # Iterate over all combinations of hyperparameters
-    for optim in OPTIM:
-        print(f"Running training with OPTIM={optim}")
-        
-        # Run training and capture the last line's training loss
-        train_loss = run_training(optim)
-        
-        if train_loss is not None:
-            log.write(f"OPTIM={optim}, TRAIN_LOSS={train_loss}\n")
-            print(f"Finished: OPTIM={optim}, TRAIN_LOSS={train_loss}")
-        else:
-            log.write(f"ERROR: OPTIM={optim}, STATUS=FAILED\n")
-            print(f"Error with: OPTIM={optim}. Marking as FAILED.")
-
-    log.write(f"Finish logging pretrain optim tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging pretrain optim tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/pretrain_warming_tuning.py b/experiments/pretrain_warming_tuning.py
deleted file mode 100644
index c41fb04..0000000
--- a/experiments/pretrain_warming_tuning.py
+++ /dev/null
@@ -1,101 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-ACCUM_VALUES = [2]
-WARMUP_EPOCHS_VALUES = [10, 15, 20]
-BASE_LR_VALUES = [5e-4]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/pretrain_lr_tuning.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(accum, warmup_epochs, base_lr):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    name = f"Bmae_deit_pretrain_pretrain_accum_{accum}_warmup_{warmup_epochs}_base_lr_{base_lr}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_train.sh", 
-        "--accum_iter", str(accum), 
-        "--warmup_epochs", str(warmup_epochs), 
-        "--blr", str(base_lr), 
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:3",
-        "--use_ema",
-        "--save_frequency", "200"
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    with open(log_path, "r") as log_file:
-        lines = log_file.readlines()
-        if lines:
-            last_line = lines[-1]
-            try:
-                # Parse the JSON line to get the training loss
-                last_entry = json.loads(last_line)
-                train_loss = last_entry.get("train_loss")
-                return train_loss
-            except json.JSONDecodeError:
-                print(f"Error parsing last line of {log_path}")
-                return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging pretrain lr tuning, at {log_current_datetime}\n")
-    print(f"Start logging pretrain lr tuning, at {log_current_datetime}")
-
-    # Iterate over all combinations of hyperparameters
-    for accum in ACCUM_VALUES:
-        for warmup_epochs in WARMUP_EPOCHS_VALUES:
-            for base_lr in BASE_LR_VALUES:
-                print(f"Running training with ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}")
-                
-                # Run training and capture the last line's training loss
-                train_loss = run_training(accum, warmup_epochs, base_lr)
-                
-                if train_loss is not None:
-                    log.write(f"ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}, TRAIN_LOSS={train_loss}\n")
-                    print(f"Finished: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}, TRAIN_LOSS={train_loss}")
-                else:
-                    log.write(f"ERROR: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}, STATUS=FAILED\n")
-                    print(f"Error with: ACCUM={accum}, WARMUP_EPOCHS={warmup_epochs}, BASE_LR={base_lr}. Marking as FAILED.")
-
-    log.write(f"Finish logging pretrain lr tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging pretrain lr tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/pretrain_weight_decay_tuning.py b/experiments/pretrain_weight_decay_tuning.py
deleted file mode 100644
index a66064c..0000000
--- a/experiments/pretrain_weight_decay_tuning.py
+++ /dev/null
@@ -1,96 +0,0 @@
-import subprocess
-import json
-import os
-from datetime import datetime
-
-# Define the hyperparameters to explore
-WEIGHT_DECAY = [0.1, 0.05, 0.01]
-
-# Output log file
-log_file = "./experiments/hyperparam_results/pretrain_weight_decay_tuning.log"
-
-# # Make sure to clear the log file before starting
-# if os.path.exists(log_file):
-#     assert os.path.getsize(log_file) == 0, f"Log file {log_file} is not empty. Please clear it before running the script."
-
-# Function to run the training process and capture the last line from log.txt
-def run_training(weight_decay):
-    # Define the output directory based on the hyperparameters
-    current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    print("current_datetime:", current_datetime)
-    name = f"Bmae_deit_pretrain_pretrain_weight_decay_{weight_decay}"
-    output_dir = f"./ckpts/{name}/{current_datetime}"
-    
-    # Run the training script using subprocess
-    command = [
-        "bash", "bash_scripts/Bmae_train.sh", 
-        "--weight_decay", str(weight_decay),
-        "--current_datetime", str(current_datetime),
-        "--name", str(name),
-        "--device", "cuda:0",
-        "--use_ema",
-        "--save_frequency", "200",
-        "--bootstrap_steps", "200",
-    ]
-    
-    # Run the command and capture the output
-    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
-    
-    # Stream the output to the terminal in real-time
-    for line in process.stdout:
-        print(line, end="")  # Print each line of stdout in real-time
-    for line in process.stderr:
-        print(line, end="")  # Print each line of stderr in real-time
-    
-    # Wait for the process to complete
-    process.wait()
-    
-    # Check if the process ran successfully
-    if process.returncode != 0:
-        print(f"Error occurred with parameters: WEIGHT_DECAY={weight_decay}")
-        return None
-    
-    # Read the last line from the log file
-    log_path = os.path.join(output_dir, "log.txt")
-    
-    with open(log_path, "r") as log_file:
-        lines = log_file.readlines()
-        if lines:
-            last_line = lines[-1]
-            try:
-                # Parse the JSON line to get the training loss
-                last_entry = json.loads(last_line)
-                train_loss = last_entry.get("train_loss")
-                return train_loss
-            except json.JSONDecodeError:
-                print(f"Error parsing last line of {log_path}")
-                return None
-
-# 确保日志文件的父目录存在
-log_dir = os.path.dirname(log_file)
-if not os.path.exists(log_dir):
-    os.makedirs(log_dir)
-
-with open(log_file, "a") as log:
-    log_current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")  # 获取当前时间并格式化为字符串
-    log.write(f"\n\n*****************************************************************\n")
-    log.write(f"Start logging pretrain weight_decay tuning, at {log_current_datetime}\n")
-    print(f"Start logging pretrain weight_decay tuning, at {log_current_datetime}")
-
-    # Iterate over all combinations of hyperparameters
-    for weight_decay in WEIGHT_DECAY:
-        print(f"Running training with WEIGHT_DECAY={weight_decay}")
-        
-        # Run training and capture the last line's training loss
-        train_loss = run_training(weight_decay)
-        
-        if train_loss is not None:
-            log.write(f"WEIGHT_DECAY={weight_decay}, TRAIN_LOSS={train_loss}\n")
-            print(f"Finished: WEIGHT_DECAY={weight_decay}, TRAIN_LOSS={train_loss}")
-        else:
-            log.write(f"ERROR: WEIGHT_DECAY={weight_decay}, STATUS=FAILED\n")
-            print(f"Error with: WEIGHT_DECAY={weight_decay}. Marking as FAILED.")
-
-    log.write(f"Finish logging pretrain weight_decay tuning, at {log_current_datetime}\n")
-    log.write(f"*****************************************************************\n\n")
-    print(f"Finish logging pretrain weight_decay tuning, at {log_current_datetime}")
\ No newline at end of file
diff --git a/experiments/run_experiments.sh b/experiments/run_experiments.sh
deleted file mode 100644
index 8680974..0000000
--- a/experiments/run_experiments.sh
+++ /dev/null
@@ -1,19 +0,0 @@
-#!/bin/bash
-
-# 定义要按顺序运行的 Python 脚本列表
-scripts=(
-    "experiments/pretrain_norm_tuning.py"
-    "experiments/pretrain_optim_tuning.py"
-)
-
-# 按顺序运行列表中的 Python 脚本
-for script in "${scripts[@]}"; do
-    echo "Running $script..."
-    if python "$script"; then
-        echo "$script completed successfully!"
-    else
-        echo "Error occurred while running $script. Skipping to the next script."
-    fi
-done
-
-echo "All scripts have been executed!"
\ No newline at end of file
diff --git a/experiments/run_experiments_half_life.sh b/experiments/run_experiments_half_life.sh
deleted file mode 100644
index 2c550b6..0000000
--- a/experiments/run_experiments_half_life.sh
+++ /dev/null
@@ -1,19 +0,0 @@
-#!/bin/bash
-
-# 定义要按顺序运行的 Python 脚本列表
-scripts=(
-    "experiments/pretrain_accum_tuning.py"
-    "experiments/pretrain_half_life_tuning.py"
-)
-
-# 按顺序运行列表中的 Python 脚本
-for script in "${scripts[@]}"; do
-    echo "Running $script..."
-    if python "$script"; then
-        echo "$script completed successfully!"
-    else
-        echo "Error occurred while running $script. Skipping to the next script."
-    fi
-done
-
-echo "All scripts have been executed!"
\ No newline at end of file
diff --git a/experiments/run_experiments_lr.sh b/experiments/run_experiments_lr.sh
deleted file mode 100644
index 7a3f413..0000000
--- a/experiments/run_experiments_lr.sh
+++ /dev/null
@@ -1,20 +0,0 @@
-#!/bin/bash
-
-# 定义要按顺序运行的 Python 脚本列表
-scripts=(
-    "experiments/pretrain_lr_tuning.py"
-    "experiments/pretrain_warming_tuning.py"
-    "experiments/pretrain_accum_tuning.py"
-)
-
-# 按顺序运行列表中的 Python 脚本
-for script in "${scripts[@]}"; do
-    echo "Running $script..."
-    if python "$script"; then
-        echo "$script completed successfully!"
-    else
-        echo "Error occurred while running $script. Skipping to the next script."
-    fi
-done
-
-echo "All scripts have been executed!"
\ No newline at end of file
diff --git a/experiments/run_finetune.sh b/experiments/run_finetune.sh
deleted file mode 100644
index 4bef49f..0000000
--- a/experiments/run_finetune.sh
+++ /dev/null
@@ -1,20 +0,0 @@
-#!/bin/bash
-
-# 定义要按顺序运行的 Python 脚本列表
-scripts=(
-    "experiments/finetune_all_norm_tuning.py"
-    "experiments/finetune_all_optim_tuning.py"
-    "experiments/finetune_all_method_tuning.py"
-)
-
-# 按顺序运行列表中的 Python 脚本
-for script in "${scripts[@]}"; do
-    echo "Running $script..."
-    if python "$script"; then
-        echo "$script completed successfully!"
-    else
-        echo "Error occurred while running $script. Skipping to the next script."
-    fi
-done
-
-echo "All scripts have been executed!"
\ No newline at end of file
diff --git a/experiments/run_linprobe.sh b/experiments/run_linprobe.sh
deleted file mode 100644
index e0231f4..0000000
--- a/experiments/run_linprobe.sh
+++ /dev/null
@@ -1,20 +0,0 @@
-#!/bin/bash
-
-# 定义要按顺序运行的 Python 脚本列表
-scripts=(
-    "experiments/linprobe_all_norm_tuning.py"
-    "experiments/linprobe_all_optim_tuning.py"
-    "experiments/linprobe_all_method_tuning.py"
-)
-
-# 按顺序运行列表中的 Python 脚本
-for script in "${scripts[@]}"; do
-    echo "Running $script..."
-    if python "$script"; then
-        echo "$script completed successfully!"
-    else
-        echo "Error occurred while running $script. Skipping to the next script."
-    fi
-done
-
-echo "All scripts have been executed!"
\ No newline at end of file
diff --git a/main_finetune.py b/main_finetune.py
index fb477e9..c3b3ab7 100644
--- a/main_finetune.py
+++ b/main_finetune.py
@@ -48,9 +48,6 @@ def get_args_parser():
                         help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')
 
     # Model parameters
-    parser.add_argument('--name', required=True,
-                        help='Name of the checkpoint')
-    
     parser.add_argument('--model', default='vit_large_patch16', type=str, metavar='MODEL',
                         help='Name of model to train')
 
@@ -134,8 +131,6 @@ def get_args_parser():
     parser.add_argument('--seed', default=0, type=int)
     parser.add_argument('--resume', default='',
                         help='resume from checkpoint')
-    parser.add_argument('--current_datetime', required=True,
-                        help='current datetime of running the code')
 
     parser.add_argument('--start_epoch', default=0, type=int, metavar='N',
                         help='start epoch')
@@ -156,9 +151,6 @@ def get_args_parser():
     parser.add_argument('--dist_on_itp', action='store_true')
     parser.add_argument('--dist_url', default='env://',
                         help='url used to set up distributed training')
-    
-    # checkpoint saving parameters
-    parser.add_argument('--save_frequency', default=20, type=int)
 
     return parser
 
@@ -223,9 +215,6 @@ def main(args):
         drop_last=False
     )
 
-    # 不同的数据增强方法
-    # Mixup: 通过将两张图像及其对应的标签按一定比例混合，生成新的训练样本。
-    # CutMix: 通过将一张图像的某个区域替换为另一张图像的对应区域，同时调整标签。
     mixup_fn = None
     mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None
     if mixup_active:
@@ -328,11 +317,10 @@ def main(args):
             log_writer=log_writer,
             args=args
         )
-        # if args.output_dir:
-        if args.output_dir and epoch != 0 and (epoch % args.save_frequency == 0 or epoch + 1 == args.epochs):
+        if args.output_dir:
             misc.save_model(
                 args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,
-                loss_scaler=loss_scaler, epoch=epoch, checkpoint_name=f"{args.name}")
+                loss_scaler=loss_scaler, epoch=epoch)
 
         test_stats = evaluate(data_loader_val, model, device)
         print(f"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%")
diff --git a/main_linprobe.py b/main_linprobe.py
index 2858332..2d3f241 100644
--- a/main_linprobe.py
+++ b/main_linprobe.py
@@ -48,14 +48,8 @@ def get_args_parser():
                         help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')
 
     # Model parameters
-    parser.add_argument('--name', required=True,
-                        help='Name of the checkpoint')
-    
     parser.add_argument('--model', default='vit_large_patch16', type=str, metavar='MODEL',
                         help='Name of model to train')
-    
-    parser.add_argument('--input_size', default=224, type=int,
-                        help='images input size')
 
     # Optimizer parameters
     parser.add_argument('--weight_decay', type=float, default=0,
@@ -95,8 +89,6 @@ def get_args_parser():
     parser.add_argument('--seed', default=0, type=int)
     parser.add_argument('--resume', default='',
                         help='resume from checkpoint')
-    parser.add_argument('--current_datetime', required=True,
-                        help='current datetime of running the code')
 
     parser.add_argument('--start_epoch', default=0, type=int, metavar='N',
                         help='start epoch')
@@ -118,9 +110,6 @@ def get_args_parser():
     parser.add_argument('--dist_url', default='env://',
                         help='url used to set up distributed training')
 
-    # checkpoint saving parameters
-    parser.add_argument('--save_frequency', default=20, type=int)
-
     return parser
 
 
@@ -141,18 +130,15 @@ def main(args):
 
     # linear probe: weak augmentation
     transform_train = transforms.Compose([
-            # RandomResizedCrop(32, interpolation=3),
-            transforms.RandomResizedCrop(args.input_size, scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic
+            RandomResizedCrop(224, interpolation=3),
             transforms.RandomHorizontalFlip(),
             transforms.ToTensor(),
-            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet mean/std
-            transforms.Normalize(mean=[0.4919, 0.4827, 0.4472], std=[0.2022, 0.1994, 0.2010])])   # CIFAR-10 mean/std
+            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])
     transform_val = transforms.Compose([
-            # transforms.Resize(256, interpolation=3),
-            # transforms.CenterCrop(224),
+            transforms.Resize(256, interpolation=3),
+            transforms.CenterCrop(224),
             transforms.ToTensor(),
-            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet mean/std
-            transforms.Normalize(mean=[0.4919, 0.4827, 0.4472], std=[0.2022, 0.1994, 0.2010])])   # CIFAR-10 mean/std
+            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])
     dataset_train = datasets.ImageFolder(os.path.join(args.data_path, 'train'), transform=transform_train)
     dataset_val = datasets.ImageFolder(os.path.join(args.data_path, 'val'), transform=transform_val)
     print(dataset_train)
@@ -206,7 +192,6 @@ def main(args):
     )
 
     if args.finetune and not args.eval:
-        print(f"Loading checkpoint from: {args.finetune}")
         checkpoint = torch.load(args.finetune, map_location='cpu')
 
         print("Load pre-trained checkpoint from: %s" % args.finetune)
@@ -234,7 +219,6 @@ def main(args):
 
     # for linear prob only
     # hack: revise model's head with BN
-    # affine=False表示这个BN层没有可学习的参数
     model.head = torch.nn.Sequential(torch.nn.BatchNorm1d(model.head.in_features, affine=False, eps=1e-6), model.head)
     # freeze all but the head
     for _, p in model.named_parameters():
@@ -293,18 +277,15 @@ def main(args):
             log_writer=log_writer,
             args=args
         )
-        
-        # if args.output_dir:
-        if args.output_dir and epoch != 0 and (epoch % args.save_frequency == 0 or epoch + 1 == args.epochs):
+        if args.output_dir:
             misc.save_model(
                 args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,
-                loss_scaler=loss_scaler, epoch=epoch, checkpoint_name=f"{args.name}")
+                loss_scaler=loss_scaler, epoch=epoch)
 
         test_stats = evaluate(data_loader_val, model, device)
         print(f"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%")
         max_accuracy = max(max_accuracy, test_stats["acc1"])
         print(f'Max accuracy: {max_accuracy:.2f}%')
-        # TODO：这个Max accuracy只会在终端输出
 
         if log_writer is not None:
             log_writer.add_scalar('perf/test_acc1', test_stats['acc1'], epoch)
diff --git a/main_pretrain.py b/main_pretrain.py
index 0835620..58a18c5 100644
--- a/main_pretrain.py
+++ b/main_pretrain.py
@@ -34,9 +34,6 @@ import models_mae
 
 from engine_pretrain import train_one_epoch
 
-import copy
-from util.ema import EMA
-from methods import *
 
 def get_args_parser():
     parser = argparse.ArgumentParser('MAE pre-training', add_help=False)
@@ -47,9 +44,6 @@ def get_args_parser():
                         help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')
 
     # Model parameters
-    parser.add_argument('--name', required=True,
-                        help='Name of the checkpoint')
-    
     parser.add_argument('--model', default='mae_vit_large_patch16', type=str, metavar='MODEL',
                         help='Name of model to train')
 
@@ -76,8 +70,6 @@ def get_args_parser():
 
     parser.add_argument('--warmup_epochs', type=int, default=40, metavar='N',
                         help='epochs to warmup LR')
-                        
-    parser.add_argument('--optim', default='AdamW', type=str)
 
     # Dataset parameters
     parser.add_argument('--data_path', default='/datasets01/imagenet_full_size/061417/', type=str,
@@ -92,10 +84,6 @@ def get_args_parser():
     parser.add_argument('--seed', default=0, type=int)
     parser.add_argument('--resume', default='',
                         help='resume from checkpoint')
-    parser.add_argument('--ckpt_name', default='checkpoint',
-                        help='name of checkpoint')
-    parser.add_argument('--current_datetime', required=True,
-                        help='current datetime of running the code')
 
     parser.add_argument('--start_epoch', default=0, type=int, metavar='N',
                         help='start epoch')
@@ -112,25 +100,6 @@ def get_args_parser():
     parser.add_argument('--dist_on_itp', action='store_true')
     parser.add_argument('--dist_url', default='env://',
                         help='url used to set up distributed training')
-    
-    # bootstrapping parameters
-    parser.add_argument('--is_bootstrapping', action='store_true')
-    parser.set_defaults(is_bootstrapping=False)
-    parser.add_argument('--bootstrap_steps', default=5, type=int)
-    parser.add_argument('--bootstrap_method', default='Last_layer', type=str)
-    parser.add_argument('--feature_layers', default=[1, 6, 12], type=int, nargs='+',
-                        help='List of feature layers (e.g., 1 6 12)')
-    parser.add_argument('--weights', default=[1, 1, 1], type=float, nargs='+',
-                        help='List of weights (e.g., 1 6 12)')
-
-    # ema parameters
-    parser.add_argument('--use_ema', action='store_true')
-    parser.set_defaults(use_ema=False)
-    parser.add_argument('--ema_decay', default=0.99, type=float)
-    # parser.add_argument('--ema_lr_decay', default=0.1, type=float)
-
-    # checkpoint saving parameters
-    parser.add_argument('--save_frequency', default=20, type=int)
 
     return parser
 
@@ -140,7 +109,6 @@ def main(args):
 
     print('job dir: {}'.format(os.path.dirname(os.path.realpath(__file__))))
     print("{}".format(args).replace(', ', ',\n'))
-    # print(args.is_bootstrapping)
 
     device = torch.device(args.device)
 
@@ -156,8 +124,7 @@ def main(args):
             transforms.RandomResizedCrop(args.input_size, scale=(0.2, 1.0), interpolation=3),  # 3 is bicubic
             transforms.RandomHorizontalFlip(),
             transforms.ToTensor(),
-            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet mean/std
-            transforms.Normalize(mean=[0.4919, 0.4827, 0.4472], std=[0.2022, 0.1994, 0.2010])])   # CIFAR-10 mean/std
+            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])
     dataset_train = datasets.ImageFolder(os.path.join(args.data_path, 'train'), transform=transform_train)
     print(dataset_train)
 
@@ -185,53 +152,19 @@ def main(args):
         drop_last=True,
     )
     
-    eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()
-    
-    if args.lr is None:  # only base_lr is specified
-        args.lr = args.blr * eff_batch_size / 256
-
-    # TODO: define the model
-    if args.is_bootstrapping:
-        model = models_mae.__dict__[args.model](
-            norm_pix_loss=args.norm_pix_loss,
-            is_bootstrapping=args.is_bootstrapping,
-            bootstrap_method=args.bootstrap_method,
-            feature_layers=args.feature_layers,
-            )
-        
-        method_class = None
-        seq_len = 64 + 1   # 多一个cls_token
-        seq_len += 1       # 使用deit，多一个distill_token
-        if args.bootstrap_method == 'Fixed_layer_fusion':
-            assert len(args.feature_layers) == len(args.weights), "Length of feature layers and weights must be equal."
-            method_class = FixedLayerFusion(args.weights)
-        elif args.bootstrap_method == 'Adaptive_layer_fusion':
-            method_class = AdaptiveLayerFusion(len(args.feature_layers))
-        elif args.bootstrap_method == 'Cross_layer_fusion':
-            method_class = CrossLayerFusion(seq_len, len(args.feature_layers)).to(device)
-        elif args.bootstrap_method == 'Gated_fusion_dynamic':
-            method_class = GatedFusionDynamic(seq_len, len(args.feature_layers)).to(device)
-        elif args.bootstrap_method == 'Cross_layer_self_attention':
-            method_class = CrossLayerSelfAttention(seq_len, len(args.feature_layers), embed_dim=192).to(device)
-        elif args.bootstrap_method == 'Cross_layer_cross_attention':
-            method_class = CrossLayerCrossAttention(seq_len, len(args.feature_layers), embed_dim=192).to(device)
-        elif args.bootstrap_method == 'Last_layer':
-            pass
-        else:
-            raise ValueError(f"Unknown bootstrap method: {args.bootstrap_method}")
-    else:
-        model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)
+    # define the model
+    model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)
 
     model.to(device)
 
-    if args.use_ema:
-        assert args.ema_decay < 1.0, "EMA decay should be less than 1.0"
-        ema_model = EMA(copy.deepcopy(model), args.ema_decay)
-        ema_model.register()
-
     model_without_ddp = model
     print("Model = %s" % str(model_without_ddp))
 
+    eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()
+    
+    if args.lr is None:  # only base_lr is specified
+        args.lr = args.blr * eff_batch_size / 256
+
     print("base lr: %.2e" % (args.lr * 256 / eff_batch_size))
     print("actual lr: %.2e" % args.lr)
 
@@ -242,189 +175,42 @@ def main(args):
         model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], find_unused_parameters=True)
         model_without_ddp = model.module
     
-    # # following timm: set wd as 0 for bias and norm layers
-    # if method_class is not None:   # 如果 method_class 存在，将其参数加入优化器
-    #     param_groups_model = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)
-    #     param_groups_method_class = optim_factory.add_weight_decay(method_class, args.weight_decay)
-    #     param_groups = param_groups_model + param_groups_method_class
-    #     # param_groups = param_groups_method_class
-    #     # print(param_groups_method_class)
-    #     # print(param_groups)
-    #     # exit(0)
-    # else:
-    #     param_groups = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)
-
-    # optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))
-    # loss_scaler = NativeScaler()
-
-    optimizer_method_class = None
-    if method_class is not None and not isinstance(method_class, FixedLayerFusion):
-        param_groups_model = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)
-        param_groups_method_class = optim_factory.add_weight_decay(method_class, args.weight_decay)
-
-        optimizer_method_class = torch.optim.AdamW(param_groups_method_class, lr=args.lr, betas=(0.9, 0.95))
-        if args.optim == "AdamW":
-            # 使用 AdamW 优化器
-            optimizer = torch.optim.AdamW(param_groups_model, lr=args.lr, betas=(0.9, 0.95))
-        elif args.optim == "SGD":
-            # 使用 SGD 优化器
-            optimizer = torch.optim.SGD(param_groups_model, lr=args.lr, momentum=0.9, weight_decay=args.weight_decay)
-        elif args.optim == "RMSprop":
-            # 使用 RMSprop 优化器
-            optimizer = torch.optim.RMSprop(param_groups_model, lr=args.lr, alpha=0.99, weight_decay=args.weight_decay)
-        else:
-            raise NotImplementedError(f"Unknown optimizer: {args.optim}")
-    else:
-        param_groups = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)
-        if args.optim == "AdamW":
-            # 使用 AdamW 优化器
-            optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))
-        elif args.optim == "SGD":
-            # 使用 SGD 优化器
-            optimizer = torch.optim.SGD(param_groups, lr=args.lr, momentum=0.9, weight_decay=args.weight_decay)
-        elif args.optim == "RMSprop":
-            # 使用 RMSprop 优化器
-            optimizer = torch.optim.RMSprop(param_groups, lr=args.lr, alpha=0.99, weight_decay=args.weight_decay)
-        else:
-            raise NotImplementedError(f"Unknown optimizer: {args.optim}")
-
+    # following timm: set wd as 0 for bias and norm layers
+    param_groups = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)
+    optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))
+    print(optimizer)
     loss_scaler = NativeScaler()
 
     misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)
-    
-    if args.is_bootstrapping:
-        epochs_per_bootstrap = args.epochs // args.bootstrap_steps
-        remaining_epochs = args.epochs % args.bootstrap_steps
-        print(f"Start training Bootstrapped MAE for {args.epochs} epochs in total, {epochs_per_bootstrap} epochs per bootstrap step")
-        start_time = time.time()
-        last_model = None
-
-        for bootstrap_iter in range(args.bootstrap_steps):
-            print(f"Starting bootstrap iteration {bootstrap_iter + 1}/{args.bootstrap_steps}")
-
-            if bootstrap_iter == args.bootstrap_steps - 1:
-                current_bootstrap_step_epochs = epochs_per_bootstrap + remaining_epochs
-            else:
-                current_bootstrap_step_epochs = epochs_per_bootstrap
-            print(f"Training for {current_bootstrap_step_epochs} epochs in this bootstrap step")
-
-            # Train for epochs_per_bootstrap epochs
-            for epoch in range(current_bootstrap_step_epochs):
-                if args.distributed:
-                    data_loader_train.sampler.set_epoch(epoch)
-                train_stats = train_one_epoch(
-                    model, data_loader_train,
-                    optimizer, device, epoch, loss_scaler,
-                    log_writer=log_writer,
-                    args=args,
-                    last_model=last_model,
-                    method_class=method_class,
-                    optimizer_method_class=optimizer_method_class,
-                )
-                if args.use_ema:
-                    ema_model.update()
-                    print("EMA model update")
-
-                if args.bootstrap_steps > 50:   # 对于bootstrap_steps很大的情况
-                    if args.output_dir and bootstrap_iter >= args.bootstrap_steps-6 and epoch + 1 == current_bootstrap_step_epochs:
-                        if args.use_ema:
-                            # Bmae with EMA
-                            ema_model.apply_shadow()
-                            misc.save_model(
-                                args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,
-                                loss_scaler=loss_scaler, epoch=epoch, checkpoint_name=f"Bmae-ema-{bootstrap_iter + 1}")
-                            ema_model.restore()
-                        else:
-                            # original Bmae
-                            misc.save_model(
-                                args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,
-                                loss_scaler=loss_scaler, epoch=epoch, checkpoint_name=f"Bmae-{bootstrap_iter + 1}")
-                else:
-                    if args.output_dir and epoch != 0 and (epoch % args.save_frequency == 0 or epoch + 1 == current_bootstrap_step_epochs):
-                        if args.use_ema:
-                            # Bmae with EMA
-                            ema_model.apply_shadow()
-                            misc.save_model(
-                                args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,
-                                loss_scaler=loss_scaler, epoch=epoch, checkpoint_name=f"Bmae-ema-{bootstrap_iter + 1}")
-                            ema_model.restore()
-                        else:
-                            # original Bmae
-                            misc.save_model(
-                                args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,
-                                loss_scaler=loss_scaler, epoch=epoch, checkpoint_name=f"Bmae-{bootstrap_iter + 1}")
-
-                if args.use_ema:
-                    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},
-                                'epoch': epoch,
-                                'bootstrap_iter': bootstrap_iter + 1,}
-                else:
-                    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},
-                                'epoch': epoch,
-                                'bootstrap_iter': bootstrap_iter + 1,}
-
-                if args.output_dir and misc.is_main_process():
-                    if log_writer is not None:
-                        log_writer.flush()
-                    with open(os.path.join(args.output_dir, "log.txt"), mode="a", encoding="utf-8") as f:
-                        f.write(json.dumps(log_stats) + "\n")
-
-            # Update target model for bootstrapping
-            if args.use_ema:
-                ema_model.apply_shadow()
-                last_model = copy.deepcopy(ema_model.model)
-                ema_model.restore()
-            else:
-                last_model = copy.deepcopy(model)
-
-            # freeze last_model
-            for param in last_model.parameters():
-                param.requires_grad = False
-
-            # # last_model = None
-            # if args.bootstrap_method == 'Cross_layer_fusion':
-            #     print(method_class.fc.weight[0][0].item())
-            #     print(method_class.fc.bias[0].item())
-            # elif args.bootstrap_method == 'Adaptive_layer_fusion':
-            #     print(method_class.weights)   # 查看参数值是否会变化
-
-            total_time = time.time() - start_time
-            total_time_str = str(datetime.timedelta(seconds=int(total_time)))
-            print(f'Finish Training MAE-{bootstrap_iter + 1}. Training time {total_time_str}')
-
-        total_time = time.time() - start_time
-        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
-        print('Bootstrapped MAE training time {}'.format(total_time_str))
-    else:
-        print(f"Start training original MAE for {args.epochs} epochs")
-        start_time = time.time()
-
-        for epoch in range(args.start_epoch, args.epochs):
-            if args.distributed:
-                data_loader_train.sampler.set_epoch(epoch)
-            train_stats = train_one_epoch(
-                model, data_loader_train,
-                optimizer, device, epoch, loss_scaler,
-                log_writer=log_writer,
-                args=args
-            )
-            if args.output_dir and epoch != 0 and (epoch % args.save_frequency == 0 or epoch + 1 == args.epochs):
-                misc.save_model(
-                    args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,
-                    loss_scaler=loss_scaler, epoch=epoch, checkpoint_name=f"{args.name}")
-
-            log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},
-                            'epoch': epoch,}
-
-            if args.output_dir and misc.is_main_process():
-                if log_writer is not None:
-                    log_writer.flush()
-                with open(os.path.join(args.output_dir, "log.txt"), mode="a", encoding="utf-8") as f:
-                    f.write(json.dumps(log_stats) + "\n")
-
-        total_time = time.time() - start_time
-        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
-        print('Original MAE training time {}'.format(total_time_str))
+
+    print(f"Start training for {args.epochs} epochs")
+    start_time = time.time()
+    for epoch in range(args.start_epoch, args.epochs):
+        if args.distributed:
+            data_loader_train.sampler.set_epoch(epoch)
+        train_stats = train_one_epoch(
+            model, data_loader_train,
+            optimizer, device, epoch, loss_scaler,
+            log_writer=log_writer,
+            args=args
+        )
+        if args.output_dir and (epoch % 20 == 0 or epoch + 1 == args.epochs):
+            misc.save_model(
+                args=args, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,
+                loss_scaler=loss_scaler, epoch=epoch)
+
+        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},
+                        'epoch': epoch,}
+
+        if args.output_dir and misc.is_main_process():
+            if log_writer is not None:
+                log_writer.flush()
+            with open(os.path.join(args.output_dir, "log.txt"), mode="a", encoding="utf-8") as f:
+                f.write(json.dumps(log_stats) + "\n")
+
+    total_time = time.time() - start_time
+    total_time_str = str(datetime.timedelta(seconds=int(total_time)))
+    print('Training time {}'.format(total_time_str))
 
 
 if __name__ == '__main__':
diff --git a/methods/__init__.py b/methods/__init__.py
deleted file mode 100644
index e63753b..0000000
--- a/methods/__init__.py
+++ /dev/null
@@ -1,7 +0,0 @@
-# __init__.py
-from .adaptive_layer_fusion import AdaptiveLayerFusion
-from .cross_layer_fusion import CrossLayerFusion
-from .fixed_layer_fusion import FixedLayerFusion
-from .cross_layer_self_attention import CrossLayerSelfAttention
-from .cross_layer_cross_attention import CrossLayerCrossAttention
-from .gated_fusion_dynamic import GatedFusionDynamic
\ No newline at end of file
diff --git a/methods/adaptive_layer_fusion.py b/methods/adaptive_layer_fusion.py
deleted file mode 100644
index fb7e18c..0000000
--- a/methods/adaptive_layer_fusion.py
+++ /dev/null
@@ -1,13 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-class AdaptiveLayerFusion(nn.Module):
-    def __init__(self, layer_num):
-        super(AdaptiveLayerFusion, self).__init__()
-        self.weights = nn.Parameter(torch.ones(layer_num))  # 可优化的权重参数，初始化为全1
-
-    def forward(self, layer_outputs):
-        normalized_weights = F.softmax(self.weights, dim=0) 
-        fused_features = sum(w * layer for w, layer in zip(normalized_weights, layer_outputs))  # shape = [batch_size, layer_dim]
-        return fused_features
\ No newline at end of file
diff --git a/methods/cross_layer_cross_attention.py b/methods/cross_layer_cross_attention.py
deleted file mode 100644
index c5cf224..0000000
--- a/methods/cross_layer_cross_attention.py
+++ /dev/null
@@ -1,35 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-class CrossLayerCrossAttention(nn.Module):
-    def __init__(self, layer_dim, layer_num, embed_dim, num_heads=3):
-        assert layer_num > 1, "layer_num must be greater than 1 in CrossLayerCrossAttention"
-        self.layer_num = layer_num
-        super(CrossLayerCrossAttention, self).__init__()
-        self.attn_layers = nn.ModuleList([nn.MultiheadAttention(embed_dim, num_heads) for _ in range(layer_num)])
-        self.fc = nn.Linear(layer_dim, layer_dim)
-
-    def forward(self, layer_outputs):
-        layer_outputs = [seq.permute(1, 0, 2) for seq in layer_outputs]  # 每个序列都变为 [seq_len, batch_size, embed_dim]
-
-        attn_outputs = []
-        for i in range(self.layer_num):
-            # 从其他序列中获取要计算的序列
-            other_sequences = [seq for j, seq in enumerate(layer_outputs) if j != i]
-            # 将其他序列拼接在一起
-            context = torch.cat(other_sequences, dim=0)  # 形状：[sum(seq_len_other), batch_size, embed_dim]
-
-            # 执行交叉注意力
-            attn_output, _ = self.attn_layers[i](layer_outputs[i], context, context)
-            attn_outputs.append(attn_output)
-
-        # 加法融合所有交叉注意力的输出
-        fused_output = sum(attn_outputs)
-        # print(fused_output.shape)
-
-        # 最后通过一个全连接层映射到最终的维度
-        fused_output = fused_output.permute(1, 2, 0)   # 调整输出维度为 [batch_size, embed_dim, seq_len]
-        fused_output = self.fc(fused_output)
-
-        return fused_output.permute(0, 2, 1)  # 返回 [batch_size, seq_len, embed_dim]
\ No newline at end of file
diff --git a/methods/cross_layer_fusion.py b/methods/cross_layer_fusion.py
deleted file mode 100644
index 5e6dba3..0000000
--- a/methods/cross_layer_fusion.py
+++ /dev/null
@@ -1,22 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-class CrossLayerFusion(nn.Module):
-    def __init__(self, seq_len, layer_num):
-        super(CrossLayerFusion, self).__init__()
-        self.fc = nn.Linear(seq_len * layer_num, seq_len)
-        # print(seq_len * layer_num, seq_len)
-        # exit(0)
-    
-    def forward(self, layer_outputs):
-        fused_features = torch.cat(layer_outputs, dim=1)
-        # print(fused_features.shape)
-        
-        fused_features = fused_features.permute(0, 2, 1)  # 调整维度为 [batch_size, embed_dim, seq_len * 3]
-        # print(fused_features.shape)
-        output = self.fc(fused_features)  # 形状：[batch_size, embed_dim, seq_len]
-        output = output.permute(0, 2, 1)  # 调整维度为 [batch_size, seq_len, embed_dim]
-        # print(output)
-
-        return output
\ No newline at end of file
diff --git a/methods/cross_layer_self_attention.py b/methods/cross_layer_self_attention.py
deleted file mode 100644
index ba5b6f8..0000000
--- a/methods/cross_layer_self_attention.py
+++ /dev/null
@@ -1,26 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-class CrossLayerSelfAttention(nn.Module):
-    def __init__(self, layer_dim, layer_num, embed_dim, num_heads=3):
-        super(CrossLayerSelfAttention, self).__init__()
-        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)
-        self.fc_seq_to_layer = nn.Linear(layer_dim * layer_num, layer_dim)  
-
-    def forward(self, layer_outputs):
-        # 假设 layer_outputs 是一个包含多个层输出的列表
-        # 转换为 [seq_len, batch_size, feature_dim] 的格式
-        layer_outputs = torch.cat(layer_outputs, dim=1)  # 形状：[seq_len, batch_size, feature_dim]
-        layer_outputs = layer_outputs.permute(1, 0, 2)  # 形状：[seq_len, batch_size, feature_dim]
-
-        # 跨层自注意力
-        # print(layer_outputs.shape)
-        attn_output, _ = self.attn(layer_outputs, layer_outputs, layer_outputs)  # 形状：[seq_len, batch_size, feature_dim]
-
-        # 使用线性层将 seq_len 转换为 layer_dim
-        attn_output = attn_output.permute(1, 2, 0)  # 调整维度为 [batch_size, feature_dim, seq_len]
-        output = self.fc_seq_to_layer(attn_output)  # 形状：[batch_size, feature_dim, layer_dim]
-        output = output.permute(0, 2, 1)  # 调整维度为 [batch_size, layer_dim, feature_dim]
-
-        return output
\ No newline at end of file
diff --git a/methods/fixed_layer_fusion.py b/methods/fixed_layer_fusion.py
deleted file mode 100644
index 75ac30b..0000000
--- a/methods/fixed_layer_fusion.py
+++ /dev/null
@@ -1,13 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-class FixedLayerFusion(nn.Module):
-    def __init__(self, weights):
-        super(FixedLayerFusion, self).__init__()
-        self.weights = torch.tensor(weights, dtype=torch.float32)
-
-    def forward(self, layer_outputs):
-        normalized_weights = F.softmax(self.weights, dim=0) 
-        fused_features = sum(w * layer for w, layer in zip(normalized_weights, layer_outputs))  # shape = [batch_size, layer_dim]
-        return fused_features
\ No newline at end of file
diff --git a/methods/gated_fusion_dynamic.py b/methods/gated_fusion_dynamic.py
deleted file mode 100644
index 62fc4e1..0000000
--- a/methods/gated_fusion_dynamic.py
+++ /dev/null
@@ -1,36 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-class GatedFusionDynamic(nn.Module):
-    def __init__(self, seq_len, layer_num):
-        super(GatedFusionDynamic, self).__init__()
-        
-        assert layer_num > 1, "layer_num must be greater than 1 in GatedFusionDynamic"
-        self.layer_num = layer_num
-        self.fc_gate = nn.ModuleList([nn.Linear(seq_len, 1) for _ in range(layer_num)])  # 门控生成每个序列的权重
-        self.fc_output = nn.Linear(seq_len, seq_len)
-
-    def forward(self, layer_outputs):
-        assert len(layer_outputs) == self.layer_num, "The number of input layer_outputs must match layer_num."
-        
-        gates = []
-        for i in range(self.layer_num):
-            layer_outputs[i] = layer_outputs[i].permute(0, 2, 1)
-            gate = torch.sigmoid(self.fc_gate[i](layer_outputs[i]))  # 计算每个序列的门控
-            gates.append(gate)
-        
-        # 初始化融合后的序列
-        fused_seq = torch.zeros_like(layer_outputs[0])  # 假设所有输入序列维度相同
-        
-        # 融合每对序列的门控信息
-        for i in range(self.layer_num):
-            for j in range(i + 1, self.layer_num):
-                # 使用门控来融合 seq_i 和 seq_j
-                gate_ij = (gates[i] + gates[j]) / 2  # 对每对序列计算门控平均值
-                fused_seq += gate_ij * (layer_outputs[i] + layer_outputs[j])  # 融合
-
-        self.fc_output(fused_seq)
-        fused_seq = fused_seq.permute(0, 2, 1)
-
-        return fused_seq
\ No newline at end of file
diff --git a/models_mae.py b/models_mae.py
index 8fc3d52..880e28f 100644
--- a/models_mae.py
+++ b/models_mae.py
@@ -18,7 +18,6 @@ from timm.models.vision_transformer import PatchEmbed, Block
 
 from util.pos_embed import get_2d_sincos_pos_embed
 
-from methods import *
 
 class MaskedAutoencoderViT(nn.Module):
     """ Masked Autoencoder with VisionTransformer backbone
@@ -26,38 +25,16 @@ class MaskedAutoencoderViT(nn.Module):
     def __init__(self, img_size=224, patch_size=16, in_chans=3,
                  embed_dim=1024, depth=24, num_heads=16,
                  decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
-                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False, 
-                 is_distill_token=False, is_bootstrapping=False, bootstrap_method='Last_layer',
-                 feature_layers=None):
+                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_pix_loss=False):
         super().__init__()
 
         # --------------------------------------------------------------------------
         # MAE encoder specifics
         self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)
         num_patches = self.patch_embed.num_patches
-        self.num_patches = num_patches
-
-        if is_bootstrapping:
-            print(bootstrap_method)
-            assert bootstrap_method in ['Last_layer', 'Fixed_layer_fusion', 'Adaptive_layer_fusion', 'Cross_layer_fusion', \
-                                        'Gated_fusion_dynamic', 'Cross_layer_self_attention', 'Cross_layer_cross_attention'], \
-                    'bootstrap_method must be one of [Last_layer, Fixed_layer_fusion, Adaptive_layer_fusion, Cross_layer_fusion,' \
-                    ' Gated_fusion_dynamic, Cross_layer_self_attention and Cross_layer_cross_attention]'
-            if bootstrap_method != 'Last_layer':
-                assert feature_layers is not None, 'feature_layers must be specified for Hierarchical layers bootstrap'
-
-        self.feature_layers = feature_layers
-        self.is_bootstrapping = is_bootstrapping
-        self.bootstrap_method = bootstrap_method
-
-        self.is_distill_token = is_distill_token
-        if is_distill_token:
-            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
-            self.distill_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
-            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, embed_dim), requires_grad=False)  # fixed sin-cos embedding
-        else:
-            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
-            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding
+
+        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
+        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding
 
         self.blocks = nn.ModuleList([
             Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)
@@ -71,10 +48,7 @@ class MaskedAutoencoderViT(nn.Module):
 
         self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))
 
-        if is_distill_token:
-            self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding
-        else:
-            self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding
+        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding
 
         self.decoder_blocks = nn.ModuleList([
             Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)
@@ -91,10 +65,10 @@ class MaskedAutoencoderViT(nn.Module):
     def initialize_weights(self):
         # initialization
         # initialize (and freeze) pos_embed by sin-cos embedding
-        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True, distill_token=self.is_distill_token)
+        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)
         self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))
 
-        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True, distill_token=self.is_distill_token)
+        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)
         self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))
 
         # initialize patch_embed like nn.Linear (instead of nn.Conv2d)
@@ -103,8 +77,6 @@ class MaskedAutoencoderViT(nn.Module):
 
         # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)
         torch.nn.init.normal_(self.cls_token, std=.02)
-        if self.is_distill_token:
-            torch.nn.init.normal_(self.distill_token, std=.02)
         torch.nn.init.normal_(self.mask_token, std=.02)
 
         # initialize nn.Linear and nn.LayerNorm
@@ -175,102 +147,24 @@ class MaskedAutoencoderViT(nn.Module):
 
         return x_masked, mask, ids_restore
 
-    def forward_encoder(self, x, mask_ratio, bootstrapping = False, method_class = None):
+    def forward_encoder(self, x, mask_ratio):
         # embed patches
         x = self.patch_embed(x)
 
-        if self.is_distill_token:
-            # add pos embed w/o cls token
-            x = x + self.pos_embed[:, 1:-1, :]
-
-            # masking: length -> length * mask_ratio
-            x, mask, ids_restore = self.random_masking(x, mask_ratio)
+        # add pos embed w/o cls token
+        x = x + self.pos_embed[:, 1:, :]
 
-            # append cls token
-            cls_token = self.cls_token + self.pos_embed[:, :1, :]
-            distill_token = self.distill_token + self.pos_embed[:, -1:, :]
-            cls_tokens = cls_token.expand(x.shape[0], -1, -1)
-            distill_tokens = distill_token.expand(x.shape[0], -1, -1)
-            x = torch.cat((cls_tokens, x, distill_tokens), dim=1)
-        else:
-            # add pos embed w/o cls token
-            x = x + self.pos_embed[:, 1:, :]
+        # masking: length -> length * mask_ratio
+        x, mask, ids_restore = self.random_masking(x, mask_ratio)
 
-            # masking: length -> length * mask_ratio
-            x, mask, ids_restore = self.random_masking(x, mask_ratio)
-
-            # append cls token
-            cls_token = self.cls_token + self.pos_embed[:, :1, :]
-            cls_tokens = cls_token.expand(x.shape[0], -1, -1)
-            x = torch.cat((cls_tokens, x), dim=1)
+        # append cls token
+        cls_token = self.cls_token + self.pos_embed[:, :1, :]
+        cls_tokens = cls_token.expand(x.shape[0], -1, -1)
+        x = torch.cat((cls_tokens, x), dim=1)
 
         # apply Transformer blocks
-        if bootstrapping:
-            if self.bootstrap_method == 'Last_layer':
-                for blk in self.blocks:
-                    x = blk(x)
-            elif self.bootstrap_method == 'Fixed_layer_fusion':
-                assert method_class is not None, 'method_class must be specified for Fixed_layer_fusion'
-                layer_outputs = []
-                for index, blk in enumerate(self.blocks):
-                    x = blk(x)
-                    if (index + 1) in self.feature_layers:
-                        layer_outputs.append(x)
-                        # features += x
-                # x = features / len(self.feature_layers)   # 相当于均匀分配各层之间的权重
-                x = method_class(layer_outputs)
-            elif self.bootstrap_method == 'Adaptive_layer_fusion':
-                assert method_class is not None, 'method_class must be specified for Adaptive_layer_fusion'
-                layer_outputs = []
-                for index, blk in enumerate(self.blocks):
-                    x = blk(x)
-                    if (index + 1) in self.feature_layers:
-                        layer_outputs.append(x)
-                x = method_class(layer_outputs)
-            elif self.bootstrap_method == 'Cross_layer_fusion':
-                assert method_class is not None, 'method_class must be specified for Cross_layer_fusion'
-                # print("Cross_layer_fusion")
-                layer_outputs = []
-                for index, blk in enumerate(self.blocks):
-                    x = blk(x)
-                    # print(x.shape)
-                    if (index + 1) in self.feature_layers:
-                        layer_outputs.append(x)
-                x = method_class(layer_outputs)
-            elif self.bootstrap_method == 'Gated_fusion_dynamic':
-                assert method_class is not None, 'method_class must be specified for Gated_fusion_dynamic'
-                # print("Gated_fusion_dynamic")
-                layer_outputs = []
-                for index, blk in enumerate(self.blocks):
-                    x = blk(x)
-                    # print(x.shape)
-                    if (index + 1) in self.feature_layers:
-                        layer_outputs.append(x)
-                x = method_class(layer_outputs)
-            elif self.bootstrap_method == 'Cross_layer_self_attention':
-                assert method_class is not None, 'method_class must be specified for Cross_layer_self_attention'
-                layer_outputs = []
-                for index, blk in enumerate(self.blocks):
-                    x = blk(x)
-                    # print(x.shape)
-                    if (index + 1) in self.feature_layers:
-                        layer_outputs.append(x)
-                x = method_class(layer_outputs)
-            elif self.bootstrap_method == 'Cross_layer_cross_attention':
-                assert method_class is not None, 'method_class must be specified for Cross_layer_cross_attention'
-                layer_outputs = []
-                for index, blk in enumerate(self.blocks):
-                    x = blk(x)
-                    # print(x.shape)
-                    if (index + 1) in self.feature_layers:
-                        layer_outputs.append(x)
-                x = method_class(layer_outputs)
-            else:
-                raise NotImplementedError(f"Unknown bootstrap method: {self.bootstrap_method}")
-        else:
-            for blk in self.blocks:
-                x = blk(x)
-        
+        for blk in self.blocks:
+            x = blk(x)
         x = self.norm(x)
 
         return x, mask, ids_restore
@@ -279,19 +173,11 @@ class MaskedAutoencoderViT(nn.Module):
         # embed tokens
         x = self.decoder_embed(x)
 
-        if self.is_distill_token:
-            # append mask tokens to sequence
-            # 相当于没mask的部分直接使用原图，最后计算loss的时候只计算mask的部分
-            mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 2 - x.shape[1], 1) # +2 for cls and distill token
-            x_ = torch.cat([x[:, 1:-1, :], mask_tokens], dim=1)  # no cls token
-            x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle
-            x = torch.cat([x[:, :1, :], x_, x[:, -1:, :]], dim=1)  # append cls token
-        else:
-            # append mask tokens to sequence
-            mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)
-            x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token
-            x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle
-            x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token
+        # append mask tokens to sequence
+        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)
+        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token
+        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle
+        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token
 
         # add pos embed
         x = x + self.decoder_pos_embed
@@ -304,60 +190,33 @@ class MaskedAutoencoderViT(nn.Module):
         # predictor projection
         x = self.decoder_pred(x)
 
-        if self.is_distill_token:
-            # remove cls and distill token
-            x = x[:, 1:-1, :]
-        else:
-            # remove cls token
-            x = x[:, 1:, :]
+        # remove cls token
+        x = x[:, 1:, :]
 
         return x
 
-    def forward_loss(self, imgs, pred, mask, last_model=None, method_class=None):
+    def forward_loss(self, imgs, pred, mask):
         """
         imgs: [N, 3, H, W]
         pred: [N, L, p*p*3]
         mask: [N, L], 0 is keep, 1 is remove, 
         """
-        # print(self.is_bootstrapping, (last_model==None))
-        if self.is_bootstrapping and last_model is not None:
-            # print("Bootstrapping")
-            target, _, _ = last_model.forward_encoder(imgs, mask_ratio=0.0, bootstrapping=True, method_class=method_class)
-            if self.is_distill_token:
-                target = nn.functional.normalize(target[:, 1:-1, :], dim=-1)
-            else:
-                target = nn.functional.normalize(target[:, 1:, :], dim=-1)
-
-            pred, _, _ = last_model.forward_encoder(self.unpatchify(pred), mask_ratio=0.0, bootstrapping=True, method_class=method_class)
-            if self.is_distill_token:
-                pred = nn.functional.normalize(pred[:, 1:-1, :], dim=-1)
-            else:
-                pred = nn.functional.normalize(pred[:, 1:, :], dim=-1)
-                
-            # print(pred.shape, target.shape)
-            loss = (pred - target) ** 2
-            loss = loss.mean(dim=-1)  # [N, L], mean loss per patch
-
-            loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches
-            return loss
-        else:
-            # print("Original")
-            target = self.patchify(imgs)
-            if self.norm_pix_loss:
-                mean = target.mean(dim=-1, keepdim=True)
-                var = target.var(dim=-1, keepdim=True)
-                target = (target - mean) / (var + 1.e-6)**.5
-
-            loss = (pred - target) ** 2
-            loss = loss.mean(dim=-1)  # [N, L], mean loss per patch
-
-            loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches
-            return loss
-
-    def forward(self, imgs, mask_ratio=0.75, last_model=None, method_class=None):
+        target = self.patchify(imgs)
+        if self.norm_pix_loss:
+            mean = target.mean(dim=-1, keepdim=True)
+            var = target.var(dim=-1, keepdim=True)
+            target = (target - mean) / (var + 1.e-6)**.5
+
+        loss = (pred - target) ** 2
+        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch
+
+        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches
+        return loss
+
+    def forward(self, imgs, mask_ratio=0.75):
         latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)
         pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]
-        loss = self.forward_loss(imgs, pred, mask, last_model, method_class)
+        loss = self.forward_loss(imgs, pred, mask)
         return loss, pred, mask
 
 
@@ -384,25 +243,8 @@ def mae_vit_huge_patch14_dec512d8b(**kwargs):
         mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
     return model
 
-def mae_vit_tiny_patch4_dec512d8b(**kwargs):
-    model = MaskedAutoencoderViT(
-        # TODO: 这里embed_dim之后调调
-        img_size=32, patch_size=4, embed_dim=192, depth=12, num_heads=3,
-        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
-        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
-    return model
-
-def mae_deit_tiny_patch4_dec512d8b(**kwargs):
-    model = MaskedAutoencoderViT(
-        # TODO: 这里embed_dim之后调调
-        img_size=32, patch_size=4, embed_dim=192, depth=12, num_heads=3,
-        decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,
-        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), is_distill_token=True, **kwargs)
-    return model
 
 # set recommended archs
 mae_vit_base_patch16 = mae_vit_base_patch16_dec512d8b  # decoder: 512 dim, 8 blocks
 mae_vit_large_patch16 = mae_vit_large_patch16_dec512d8b  # decoder: 512 dim, 8 blocks
 mae_vit_huge_patch14 = mae_vit_huge_patch14_dec512d8b  # decoder: 512 dim, 8 blocks
-mae_vit_tiny_patch4 = mae_vit_tiny_patch4_dec512d8b  # decoder: 512 dim, 8 blocks
-mae_deit_tiny_patch4 = mae_deit_tiny_patch4_dec512d8b  # decoder: 512 dim, 8 blocks
diff --git a/models_vit.py b/models_vit.py
index f720c58..2244a17 100644
--- a/models_vit.py
+++ b/models_vit.py
@@ -20,16 +20,9 @@ import timm.models.vision_transformer
 class VisionTransformer(timm.models.vision_transformer.VisionTransformer):
     """ Vision Transformer with support for global average pooling
     """
-    def __init__(self, global_pool=False, is_distill_token=False, **kwargs):
+    def __init__(self, global_pool=False, **kwargs):
         super(VisionTransformer, self).__init__(**kwargs)
 
-        self.is_distill_token = is_distill_token
-        if self.is_distill_token:
-            self.distill_token = nn.Parameter(torch.zeros(1, 1, kwargs['embed_dim']))
-            nn.init.trunc_normal_(self.distill_token, std=.02)
-            self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.num_patches + 2, kwargs['embed_dim']))
-            nn.init.trunc_normal_(self.pos_embed, std=.02)
-
         self.global_pool = global_pool
         if self.global_pool:
             norm_layer = kwargs['norm_layer']
@@ -42,33 +35,20 @@ class VisionTransformer(timm.models.vision_transformer.VisionTransformer):
         B = x.shape[0]
         x = self.patch_embed(x)
 
-        if self.is_distill_token:
-            cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks
-            distill_tokens = self.distill_token.expand(B, -1, -1)
-            x = torch.cat((cls_tokens, x, distill_tokens), dim=1)
-            x = x + self.pos_embed
-            x = self.pos_drop(x)
-        else:
-            cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks
-            x = torch.cat((cls_tokens, x), dim=1)
-            x = x + self.pos_embed
-            x = self.pos_drop(x)
+        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks
+        x = torch.cat((cls_tokens, x), dim=1)
+        x = x + self.pos_embed
+        x = self.pos_drop(x)
 
         for blk in self.blocks:
             x = blk(x)
 
         if self.global_pool:
-            if self.is_distill_token:
-                x = x[:, 1:-1, :].mean(dim=1)  # global pool without cls token and distill token
-            else:
-                x = x[:, 1:, :].mean(dim=1)  # global pool without cls token
+            x = x[:, 1:, :].mean(dim=1)  # global pool without cls token
             outcome = self.fc_norm(x)
         else:
             x = self.norm(x)
-            if self.is_distill_token:
-                outcome = (x[:, 0] + x[:, -1]) / 2  # TODO: average of cls token and distill token
-            else:
-                outcome = x[:, 0]
+            outcome = x[:, 0]
 
         return outcome
 
@@ -91,16 +71,4 @@ def vit_huge_patch14(**kwargs):
     model = VisionTransformer(
         patch_size=14, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, qkv_bias=True,
         norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
-    return model
-
-def vit_tiny_patch4(**kwargs):
-    model = VisionTransformer(
-        img_size=32, patch_size=4, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,
-        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
-    return model
-
-def deit_tiny_patch4(**kwargs):
-    model = VisionTransformer(
-        img_size=32, patch_size=4, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,
-        norm_layer=partial(nn.LayerNorm, eps=1e-6), is_distill_token=True, **kwargs)
     return model
\ No newline at end of file
diff --git a/try_cuda.py b/try_cuda.py
deleted file mode 100644
index 97b393b..0000000
--- a/try_cuda.py
+++ /dev/null
@@ -1,3 +0,0 @@
-import torch
-print(torch.cuda.is_available())  # 应该返回 True
-print(torch.cuda.get_device_name(0))  # 显示你的 GPU 名称
diff --git a/util/__pycache__/lr_sched.cpython-39.pyc b/util/__pycache__/lr_sched.cpython-39.pyc
deleted file mode 100644
index 10f6866..0000000
Binary files a/util/__pycache__/lr_sched.cpython-39.pyc and /dev/null differ
diff --git a/util/__pycache__/misc.cpython-39.pyc b/util/__pycache__/misc.cpython-39.pyc
deleted file mode 100644
index 72d4b14..0000000
Binary files a/util/__pycache__/misc.cpython-39.pyc and /dev/null differ
diff --git a/util/__pycache__/pos_embed.cpython-39.pyc b/util/__pycache__/pos_embed.cpython-39.pyc
deleted file mode 100644
index 6b362fe..0000000
Binary files a/util/__pycache__/pos_embed.cpython-39.pyc and /dev/null differ
diff --git a/util/crop.py b/util/crop.py
index bb2d97f..fcb2612 100644
--- a/util/crop.py
+++ b/util/crop.py
@@ -21,7 +21,7 @@ class RandomResizedCrop(transforms.RandomResizedCrop):
     """
     @staticmethod
     def get_params(img, scale, ratio):
-        width, height = F.get_image_size(img)
+        width, height = F._get_image_size(img)
         area = height * width
 
         target_area = area * torch.empty(1).uniform_(scale[0], scale[1]).item()
diff --git a/util/ema.py b/util/ema.py
deleted file mode 100644
index 7b0568e..0000000
--- a/util/ema.py
+++ /dev/null
@@ -1,35 +0,0 @@
-class EMA():
-    def __init__(self, model, decay):
-        self.model = model
-        self.decay = decay
-        self.shadow = {}     # just a dictionary, not a model
-        self.backup = {}
-
-    def register(self):
-        for name, param in self.model.named_parameters():
-            if param.requires_grad:
-                self.shadow[name] = param.data.clone()
-                # print(name)
-
-    def update(self):
-        for name, param in self.model.named_parameters():
-            if param.requires_grad:
-                assert name in self.shadow
-                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]
-                self.shadow[name] = new_average.clone()
-                # print(param.data, self.shadow[name])
-
-    def apply_shadow(self):
-        for name, param in self.model.named_parameters():
-            if param.requires_grad:
-                assert name in self.shadow
-                self.backup[name] = param.data
-                # print(param.data, self.shadow[name])
-                param.data = self.shadow[name]
-
-    def restore(self):
-        for name, param in self.model.named_parameters():
-            if param.requires_grad:
-                assert name in self.backup
-                param.data = self.backup[name]
-        self.backup = {}
\ No newline at end of file
diff --git a/util/methods.py b/util/methods.py
deleted file mode 100644
index d7d5781..0000000
--- a/util/methods.py
+++ /dev/null
@@ -1,112 +0,0 @@
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-
-class FixedLayerFusion(nn.Module):
-    def __init__(self, weights):
-        super(FixedLayerFusion, self).__init__()
-        self.weights = torch.tensor(weights, dtype=torch.float32)
-
-    def forward(self, layer_outputs):
-        normalized_weights = F.softmax(self.weights, dim=0) 
-        fused_features = sum(w * layer for w, layer in zip(normalized_weights, layer_outputs))  # shape = [batch_size, layer_dim]
-        return fused_features
-
-class AdaptiveLayerFusion(nn.Module):
-    def __init__(self, layer_num):
-        super(AdaptiveLayerFusion, self).__init__()
-        self.weights = nn.Parameter(torch.ones(layer_num))  # 可优化的权重参数，初始化为全1
-
-    def forward(self, layer_outputs):
-        normalized_weights = F.softmax(self.weights, dim=0) 
-        fused_features = sum(w * layer for w, layer in zip(normalized_weights, layer_outputs))  # shape = [batch_size, layer_dim]
-        return fused_features
-    
-class GatedFusionDynamic(nn.Module):
-    def __init__(self, seq_len, layer_num):
-        super(GatedFusionDynamic, self).__init__()
-        
-        assert layer_num > 1, "layer_num must be greater than 1 in GatedFusionDynamic"
-        self.layer_num = layer_num
-        self.fc_gate = nn.ModuleList([nn.Linear(seq_len, 1) for _ in range(layer_num)])  # 门控生成每个序列的权重
-        self.fc_output = nn.Linear(seq_len, seq_len)
-
-    def forward(self, layer_outputs):
-        assert len(layer_outputs) == self.layer_num, "The number of input layer_outputs must match layer_num."
-        
-        gates = []
-        for i in range(self.layer_num):
-            layer_outputs[i] = layer_outputs[i].permute(0, 2, 1)
-            gate = torch.sigmoid(self.fc_gate[i](layer_outputs[i]))  # 计算每个序列的门控
-            gates.append(gate)
-        
-        # 初始化融合后的序列
-        fused_seq = torch.zeros_like(layer_outputs[0])  # 假设所有输入序列维度相同
-        
-        # 融合每对序列的门控信息
-        for i in range(self.layer_num):
-            for j in range(i + 1, self.layer_num):
-                # 使用门控来融合 seq_i 和 seq_j
-                gate_ij = (gates[i] + gates[j]) / 2  # 对每对序列计算门控平均值
-                fused_seq += gate_ij * (layer_outputs[i] + layer_outputs[j])  # 融合
-
-        self.fc_output(fused_seq)
-        fused_seq = fused_seq.permute(0, 2, 1)
-
-        return fused_seq
-    
-class CrossLayerSelfAttention(nn.Module):
-    def __init__(self, layer_dim, layer_num, embed_dim, num_heads=3):
-        super(CrossLayerSelfAttention, self).__init__()
-        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)
-        self.fc_seq_to_layer = nn.Linear(layer_dim * layer_num, layer_dim)  
-
-    def forward(self, layer_outputs):
-        # 假设 layer_outputs 是一个包含多个层输出的列表
-        # 转换为 [seq_len, batch_size, feature_dim] 的格式
-        layer_outputs = torch.cat(layer_outputs, dim=1)  # 形状：[seq_len, batch_size, feature_dim]
-        layer_outputs = layer_outputs.permute(1, 0, 2)  # 形状：[seq_len, batch_size, feature_dim]
-
-        # 跨层自注意力
-        # print(layer_outputs.shape)
-        attn_output, _ = self.attn(layer_outputs, layer_outputs, layer_outputs)  # 形状：[seq_len, batch_size, feature_dim]
-
-        # 使用线性层将 seq_len 转换为 layer_dim
-        attn_output = attn_output.permute(1, 2, 0)  # 调整维度为 [batch_size, feature_dim, seq_len]
-        output = self.fc_seq_to_layer(attn_output)  # 形状：[batch_size, feature_dim, layer_dim]
-        output = output.permute(0, 2, 1)  # 调整维度为 [batch_size, layer_dim, feature_dim]
-
-        return output
-    
-
-class CrossLayerCrossAttention(nn.Module):
-    def __init__(self, layer_dim, layer_num, embed_dim, num_heads=3):
-        assert layer_num > 1, "layer_num must be greater than 1 in CrossLayerCrossAttention"
-        self.layer_num = layer_num
-        super(CrossLayerCrossAttention, self).__init__()
-        self.attn_layers = nn.ModuleList([nn.MultiheadAttention(embed_dim, num_heads) for _ in range(layer_num)])
-        self.fc = nn.Linear(layer_dim, layer_dim)
-
-    def forward(self, layer_outputs):
-        layer_outputs = [seq.permute(1, 0, 2) for seq in layer_outputs]  # 每个序列都变为 [seq_len, batch_size, embed_dim]
-
-        attn_outputs = []
-        for i in range(self.layer_num):
-            # 从其他序列中获取要计算的序列
-            other_sequences = [seq for j, seq in enumerate(layer_outputs) if j != i]
-            # 将其他序列拼接在一起
-            context = torch.cat(other_sequences, dim=0)  # 形状：[sum(seq_len_other), batch_size, embed_dim]
-
-            # 执行交叉注意力
-            attn_output, _ = self.attn_layers[i](layer_outputs[i], context, context)
-            attn_outputs.append(attn_output)
-
-        # 加法融合所有交叉注意力的输出
-        fused_output = sum(attn_outputs)
-        # print(fused_output.shape)
-
-        # 最后通过一个全连接层映射到最终的维度
-        fused_output = fused_output.permute(1, 2, 0)   # 调整输出维度为 [batch_size, embed_dim, seq_len]
-        fused_output = self.fc(fused_output)
-
-        return fused_output.permute(0, 2, 1)  # 返回 [batch_size, seq_len, embed_dim]
\ No newline at end of file
diff --git a/util/misc.py b/util/misc.py
index 088c3d8..ad9a786 100644
--- a/util/misc.py
+++ b/util/misc.py
@@ -18,8 +18,7 @@ from pathlib import Path
 
 import torch
 import torch.distributed as dist
-# from torch._six import inf
-from torch import inf
+from torch._six import inf
 
 
 class SmoothedValue(object):
@@ -293,11 +292,11 @@ def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:
     return total_norm
 
 
-def save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler, checkpoint_name):
+def save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler):
     output_dir = Path(args.output_dir)
     epoch_name = str(epoch)
     if loss_scaler is not None:
-        checkpoint_paths = [output_dir / (f'{checkpoint_name}-%s.pth' % epoch_name)]
+        checkpoint_paths = [output_dir / ('checkpoint-%s.pth' % epoch_name)]
         for checkpoint_path in checkpoint_paths:
             to_save = {
                 'model': model_without_ddp.state_dict(),
@@ -310,7 +309,7 @@ def save_model(args, epoch, model, model_without_ddp, optimizer, loss_scaler, ch
             save_on_master(to_save, checkpoint_path)
     else:
         client_state = {'epoch': epoch}
-        model.save_checkpoint(save_dir=args.output_dir, tag=f"{checkpoint_name}-%s" % epoch_name, client_state=client_state)
+        model.save_checkpoint(save_dir=args.output_dir, tag="checkpoint-%s" % epoch_name, client_state=client_state)
 
 
 def load_model(args, model_without_ddp, optimizer, loss_scaler):
diff --git a/util/pos_embed.py b/util/pos_embed.py
index 67dce23..6acf8bd 100644
--- a/util/pos_embed.py
+++ b/util/pos_embed.py
@@ -17,7 +17,7 @@ import torch
 # Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py
 # MoCo v3: https://github.com/facebookresearch/moco-v3
 # --------------------------------------------------------
-def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, distill_token=False):
+def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
     """
     grid_size: int of the grid height and width
     return:
@@ -32,8 +32,6 @@ def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, distill_token
     pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
     if cls_token:
         pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
-    if distill_token:
-        pos_embed = np.concatenate([pos_embed, np.zeros([1, embed_dim])], axis=0)
     return pos_embed
 
 
